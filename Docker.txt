[KANIKO + BUILDKIT] --> daemonless containerization tools usually used in CI/CD(jenkins) to build container images. 
Docker is an opensource containerization platform for developing, shipping and running application.
It is designed to solve problem statement of "it works on my machine" i.e code portability.
its client side pgm
act as service
social networking platform

container: Is a standerd unit of software that packages up code, all its dependancies, config, processes, networking, 
os(some chunks) so the application runs quickly and relaibly from one platform to another. 
portable artifcat, easily shared and moved around.
simple words "container is a process running in standalone env with its own space and resources through cgroups and namespaces. its like light weight vm. 
control groups(How much we can use): allocates and limits resources such as cpu,memory thats are used by container. 
namespaces(what we can use): to provide container an isolated workspace((fs,network,storage etc) to run processes. pid,net,ipc,mnt,uts <kernal and version identfier>

Note: containers root user does not have root privilage on host. 
container runtime: also called as container engine, its a software that is responsible for running containers. (docker,containerd,rtc,podman etc)
create--> run--> paused--> stopped--> deleted	--> life cycle of container. 

Docker-client
    |			through /var/run/docker.sock file
docker-daemon
    | 
local image storage    <------------> DockerHub

/var/lib/docker 		--> docker home
/etc/docker/daemon.json		--> docker home,private registry configured 


Virtual machine 			vs 				Docker

app     | app    |app    |                                        |app       |app    |app    |
config  |config  |config |                                        |config    |config |config |
guest OS|Guest OS|gues OS|                                        |bin/lib   |contnr2|contnr3|
----------------------------                                      ---------------------------------
        Hypervisor          	--> h/w virtualization             Docker Engine layer (mainly Linux)		--> uses os virtualization
----------------------------                                      -----------------------------------
      Operating system 							 Operating system
---------------------------                                       ------------------------------------
        Hardware                                                            Hardware
---------------------------                                       -----------------------------------
1. heavy weight(GB's) i.e creates standalone                      1. light weight(as les as 5MB) i.e it bundles application dependacies
os on top of os                                                    and use some chunks of kernal to interact with underlying os
2. resource hungry						  2. Not a resource hungry
3. starts and run slow(starts in minute)			  3. Starts and run much fast (starts in second)
  

Hypervisor: its a software that creates and runs virtual machines. 
1. bare metal hypervisor (runs directly on hosts hardware. ex: microsoft hyper-v)
2. hosted hypervisor (runs as software layer on os. ex: virtual box)

Running a container:
Busybox - very small, light weight layer of linux
outside world of container does not know whats happening inside container unless explicitely defined. 
container is very very separate thing. 

Syntax: docker run [option] IMAGE:[TAG] [CMD] [ARG]
ex: docker run busybox echo hello world				--> docker run creates and runs container, behind the scenes its runc that is doing the process. 

docker run:											docker create + start
Pulls the specified image from the registry if not present locally.		1. Requires the image to be present locally; it doesn't automatically pull the image.
Creates and starts a container in one command.					2. Involves two separate commands to create and start the container.
Can automatically remove the container after it exits if --rm flag is used	3. Doesn't automatically remove the container after it exits. (Can't use --rm)
	
[OPTION]
-it -> gateway to interact with container. 
-i -> interactive
-t -> tty to give formattable terminal
-d -> detached mode that is to run container at background
--name -> name the container
--restart -> specify when container should restart automatically [values: no, on-failure, always, unless-stopped]
-p <host port>:<container port> -> expose containers port by mapping it to host port. (can use multiple times)
--rm -> automatically remove conatiner when it exits. cannot be used with --restart
--memory -> provide hard limit for container
--memory-reservation -> soft limit, used when exceeds hard limit
docker ps --all -> shows all container

ctl + d to exit container
docker start/stop/kill/rm imageid or containername
stop -> allows container to finish its task and stop
kill -> tells container to exit no matter what
rm - before rm container, it must be stopped 

run -> create and start container
exec -> execute command in running container


Upgrading docker engine:
1. install higher version docker
sudo yum install docker-ce=<verson> docker-ce-cli=<version>
downgrading docker:
1. stop docker service
2. uninstall docker-ce and docker-ce-cli
3. install lower docker version 
4. start docker service


Docker images:
Docker image is a immutable file containing the code and components needed to run soft in container. 
Container and images use layered file system. and each layer contains only the diff from previous layer.
image consist of one or more read only layers while container adds one additional writable layer.		--> called copy on Write (CoW)

         ----------------------------------
         |writable container layer         |
  ------------------------------           |
         |   web app           |           |
  image  |   python            |  conatiner|
         |   base os           |           |
  ------------------------------------------
Layered fs allows multiple images and containers to share same layers results in
1. smaller storage footprints
2. faster image build
3. faster image transfer

docker [management cmd] [sub-cmd]
docker info		--> provides docker info
docker image --help	--> shows all cmmands used with image
docker login -u <username> <url>		--> if you push/pull from private repo

Note: docker pull has max 200 request per 6 hours. 

docker search <imagename>		--> search available images in dockerhub
docker image pull IMAGE:[TAG]        -> download docker images
docker image history IMAGE         -> list the layers used to build image
docker image ls                    -> list the images
docker images inspect IMAGE         --> get detailed info about image
docker image rm IMAGE or docker image rmi IMAGE    -> remove image	 --> rm is general removal command (for containers,volume etc) but rmi is image specific 
docker image rm -f IMAGE   -> remove all tags and delete image. Note -> If we delete image which is being referrenced by running 
container then it will delete tag but not actual image turning into dangling image 
(which has no container + tag refer but consume space on system)

Image cleanup: 
docker system df   --> get info about disk usage on system
docker system df -v --> detailed info
docker sytem prune 	--> delete images/containers not used
docker image prune   -> to delete dangling images (images with no referrnce to tags or containers)
docker image prune -a --> remove all unused images (not used by container)
docker image save <image name> > or -o tarname.tar		--> when you dont have private or public repo and want to share image to other teams. 

docker load < imagename.tar	--> to load image tar

Note: dockerhub credentails are stored under $HOME/.docker/config.json	--> auth: base64 encoded value for username and password. you can decode using 
echo "encoded value" | base64 -d	--> it will show credential.
auth will be empty once you are exited from docker. 


------Docker container commands------
docker ps -a 					--> shows all containers
docker container rm <ID>			--> remove container. can't remove running container use -f if want
docker run -d --name=<name> <image name>	--> create and start container in detached mode with cntainer name
docker log -f <container id>			--> check runtime logs like tail in linux. -f: follow
docker rm -f ${docker ps -aq}			--> remove all containers forcefully. -q shoes only ID
docker rm $(docker ps -aqf status=exited)	--> remove only exited containers
docker ps -q					--> shows only running containers. 

docker cp source destination		--> copy files from host to container & vice-versa <docker cp nginx.conf web:/temp> --> seems like scp
docker stats				--> shows memory,cpu consumption of container. 

Note: container goes into exited state when container fails/completes or container asking for something which we are not giving. 
docker run -it ubantu			--> start ubantu and provide interactive mode(STD_IN terminal) with -i and tty terminal with -t (STD_OUT terminal)
ctr+pq					--> to come out of container without exit

docker exec container_ID command		--> execute the command inside running container. 

docker stop: gracefully(grace period is 10sec) shutdown container by sending SIGTERM signal to the main process running inside container. 
docker kill: immediately stops or kills container by sending SIGKILL signal to main process. no grace period like docker stop. 

docker top <container name>		--> get pid of container. you can kill it from host machine too. 
docker container inspect		--> get info about container (IP,port,cmd etc)

doker run -d -p 90:80 --name=web nginx	--> port mapping frm host to container.
docker commit <container_ID> <new name of image>		--> create new image out of running container. 
docker attach container_id		--> run container in foreground mode

Dockerfiles: helps to create our own images. Dockerfile is a set of instructions which are used to construct an image.
(base image,default command,application(.war/ear file),dependancies (runtime)). 
Note: 99% dockerfile starts with FROM, but only ARG can come before FROM in some cases. always use alpine or slim version of base image to reduce image size. 
Google distroless images(find in gcr.io repo)  are speciliazed container images with only apps and its dependancies without any unnecessary extras like pckg manager or shell. docker always looks for Dockerfile to build however if you have diff name then use -f docerfilename

these instructions are called directives.
FROM  -> sets base image + must be first dir in file
LABEL -> defines author name or email address
ARG  -> temp variable used while building an image. Post img built, the values of ARG are no longer available or accessible by processes running inside the container.
use --build-arg option to override it while building image.
ENV  ->  set env variables which is used at runtime(container). use -e option to override it while launging container.
WORKDIR  -> set current working dir for subsequet directives ADD, COPY, CMD, ENTRYPOINT etc + can use multiple times. (create + cd)
RUN  -> commands to be executed while building image.[inside image]
run commands has 2 forms 1.shell form(creates shell and excutes cmd)	2. executable form (executes binary directly) 
ex: 
1. RUN echo "hello world"
2. RUN ["/bin/echo","hello world"]
each RUN command adds a layer in image so its bad practice to use multiple RUN to execute muliple commands rather use \ to separate shell commands in single run. 
ex: RUN echo "Hello World" && \
        apt-get update && \
        apt-get install utils -y

Note: executable form reduce the chances of shell-related vulnerabilities and potential security risks such as shell injection attacks.

COPY ->  copy file from host to image
ADD -> similar to COPY but also pull files using URL and extract archive into loose file in images
EXPOSE -> port to be published. port number must be of application. EXPOSE is just for documentation, it will not open the port in container. 
VOLUME -> user persistent volume from host machine in containers. used for configuration files not for main files which apps need
CMD  -> sets default commands and/or parameters which can be overridden when the container is run. can have multiple CMD but last one referred.
ENTRYPOINT --> specifies the main command that will be executed when the container starts. CMD just pass a argument/parameter to an entrypoint. 
ENTRYPOINT + CMD if given both in dockerfile. use --entrypoint=<cmd> to replace entrypoint. only single entrypoint allowed. 

STOPSIGNAL -> specify singal that will use to stop container
HEALTHCHECK --> specify cmd to check container health. health checks are executed periodically by docker itself. if returns 200 then exit status 0 if app crash then exit status 1
ex: HEALTHCHECK CMD curl --fail http://<url> or Heath entrypoint || exit 1

most common: FROM,WORKDIR,COPY,RUN,CMD,ENTRYPOINT

docker context: current dir contents like dockerfile,necessary artifacts are sent to docker daemon to build image. The first thing build process does is send the entire context(current woring dir contents) recursively to daemon as tarball. 

----
ex: build python-flask docker image
ARG PYTHON_VERSION=3.8
FROM: python:${PYTHON_VERSION}
LABEL author="Shantayya Swami"			--> key=value 
WORKDIR /app
COPY . .
RUN pip install -r requirements.txt
EXPOSE 3000
CMD python ./index.py

docker build -t TAG .   <-t tag and . is dir where dockerfile resides> <--build-arg PYTHON_VERSION=latest if no value provided in dockerfile>
docker run IMAGE

----
ex: build java apps docker image
FROM openjdk:8/11-alpine
WORKDIR /app
COPY /target/demo-0.0.1-SNAPSHOT.jar/ demo.jar
CMD["java","-jar","demo.jar"]

docker build -t demoapp/0.1 . 		--> this will create docker image from dockerfile
docker run demoapp/0.1 hostname		--> here CMD will replace with hostname so app will not execute so better to use ENTRYPOINT for main commands

Note: use can --no-cache if docker dont want to use cache to build image. 
 
-----

Building efficient docker images:
general tips:
1. Put things that are unlikely to change on lower-level layers so to use caching
2. Dont create unnecessary layers to optimize docker images in terms of size,performance,security and maintainabiity
3. Avoid including any unnecessary files, packages in images

you can acheive in 3 ways:
1. Multistage builds
2. alpines base images			--> minimal and simple plus have minimal shell env and package manager
3. Distroless base images		--> minimal and more secure without any shell env and package manager


Multi-Stage builds  -> Have >1 FROM directive in dockerfile and each FROM directive starts new stage.
each stage begins completely new set of FS layer, allowing you to selectively copy only files that you need from previous layer.
using --from flag to copy files from previous layer.
In multistage builds previous stage creates temp build env and gets deleted once image created.

ex: COPY --from=0 ...
or
FROM <base-image> AS stage1
---
FROM <base-image> AS stage2
COPY --from=stage1 ...
 
Ex: build java apps image 
FROM maven:3.8.4-openjdk-17 as BUILD
WORKDIR /app
COPY . .
RUN mvn clean package 

FROM openjdk:8-apline
WORKDIR  /app
COPY --from=BUILD /app/target/demo.jar demo.jar
ENTRYPOINT["java","-jar","demo.jar"]


ex2:
FROM git-alpine as gitclone 
WORKDIR /app
git clone <url>

FROM maven:alpine as build
WORKDIR /app
COPY --from=gitclone /app/target/employee-application/pom.xml .
COPY --from=gitclone /app/target/employee-application/src src
RUN mvn package

FROM tomcat:9.0 as dockerize
COPY --from=build /app/target/*.war /usr/local/tomcat/webapps
EXPOSE 8080

docker build -t shantayyaswami/webapp:v1 . --target=build		--> if we inlcude target means build image til specific build stage

-------container security-------------
while building container images we often concern with 2 things
1. The security(through scanning each image layer) --> image layer scanning tools clair,Trivy,Anchore
2. Size of the image(Multistage build or distroless images) 		--> distroless or minimul size images like alpine reduce attack surface. 


---best practices--
1. Do not run container as ROOT
2. avoid copying unnecessary files
3. marge layers						
4. using alpine or distroless images as base image
5. using multistage builds
6. health checks
7. avoid exposing unnecessary ports
8. hardcoding credentials

Flattening an Image: sometimes image with few layers perform better and in few cases, you may want to take an image with many layers and flatten them into single layer.
1. run a container from image
docker run -d --name flat_container noflat
2. export runnig container(step1) to an archive using docker export
docker export flat_container > flat.tar
3. import an archive as image using docker import
cat flat.tar | docker import - flat:latest


stateful and stateless applicatons: 
stateful applications hold onto your data across different sessions(like shopping cart thats retains items) while stateless applications does not remember your 
past interactions and treat each interaction as new one. 


--------------------Storage and Volumes: (/var/lib/docker/volumes)---------------------------
image consist of read only layers and when we start container writable layer gets added. writable layer gets deleted when container deleted. To persist changes made in writable layer use Storage and volumes

Storage drivers(Graph drivers):
provides framework for managing the temp, internal storage of containers writable layer. supports variety of storage drivers
1. Overlay2 - File based storage. default for ubantu and CentOs 8+ + efficient for reads
2. devicemapper - block storage + efficient for lot of writes + default for centOS7-
supports 2 modes: 
		1. loop_lvm mode: simulates an additional physical disk using files on local disk + X requied add storage device
+ bad performance + only used for testing.
		2. direct_lvm mode: req an add device + good performance + used for prod
3. aufs - file storage

docker info  ---> shows which storage driver used
Note: Docker automatically select SD based on env + can override with below option
1. set --storage-driver flag while docker start
2. set 'storage-driver' value in /etc/docker/daemon.json 

docker storage consist of layers + you can find location of layered data using ---docker container inspect <conatinrid>---

docker volume: when mounting an external storage to container, use either bind mount or volume
1. Bind mounts:
---mount specific path(with /,$PWD,./) anywhere on host machine to container. 
---not portable 
---depend on host machines FS and dir structure 
2. Named mounts (Volume): 
---storage data on host FS + but storage location is managed by docker.
---More portable 
---can mount same volume to multiple containers.

syntax: docker run --mount [key=value] ...
type = bind, volume,tmpfs					
source/src=volume name or bind mount path
destination/dst/target=path to mount inside container
echo Hello,World -> messege/message.txt
ex: docker run --mount type=bind,src=/home/cloud_user/message,dst=/root,readonly busybox cat /root/message.txt
docker run --mount type=volume,src=my-volume,dst=/root busybox sh -c 'echo Hello > /root/message.txt && cat /root/message.txt'

-v syntax: docker run -v SOURCE:DESTINATION:[OPTIONS]
SOURCE: if this is volume name it will create volume + if this is path then it will create bind mount.
DESTINATION: location to mount the data inside container
OPTIONS: comma separated list of cmnds like ro for readonly 
ex: docker volume create my-volume
docker run -v /home/cloud_user/message:/root:ro busybox cat /root/message.txt		--> create read only mount when multiple containers using it.  
docker run -v my-volume:/root busybox cat /root/mesage.txt				--> my-volume mount point created under docker-home/volume/ dir

mount same volume to multiple container:
docker run --name container1 --mount src=shared-volume ...
docker run --name container2 --mount src=shared-volume ...
or
docker run --name container2 --volumes-from container1 <image>

manage volumes: 
docker volume create VOLUME_NAME
docker volume ls 
docker volume inspect VOLUME_NAME
doker volume rm VOLUME_NAME
docker volume prune				--> volume without data. 

-----setting env variables for container----
docker run --rm -e NAME=DEVOPS nginx:latest env				--> shows env variables from nginx container
docker run --rm --env-file envfile.txt	nginx:latest env		--> if want to set multiple env variables then use env file. 


------------------Docker Networking: (main purpose is to have an isolation)----------------
Bridge network: joins two separate computer networks. works at layer2(ethernet) of OSI model
Docker creates 3 networks when installed: bridge,none and host. when you start docker, default bridge network (docker01) creates automatically and all newly started containers connect to it unless speficied 

Note: docker containers never talk to eac other on IP address rather docker has enbedded DNS(127.0.0.11) which used by containers to connect each other(custom network). not applicable for default networks. 
IP address consist of 4 octets 
ex: 172.17.0.0/16	--> here 172.17 is network subnet and 2^16 are devices. 
  n/w part|host part
containers on the default bridge network can only access other container through IP address. as default bridge network does not have DNS resolver. 

	container1		container2		container3
	eth0			eth1			eth2
	 ↓			 ↓			 ↓
	 ↓______________________ ↓______________________ ↓
        | veth			veth			veth|
	|  ↓^                     ↓^                     ↓  |
	|  -------------------docker01-----------------^    |						--> default bridge network
	|_________________________↓_________________________|
				  ↓ (use NAT to convert container private IP to host public IP)
				Eth0 (Host interface)   --> to connect to outside world
brctl show				--> shows virtual eth assigned by docker01 to avail containers 

ex: docker network create my_net				--> creates custom bridge network. 
    docker run -d --net my_net nginx:latest --name c1
    docker run -d --net my_net nginx:latest --name c2
    docker exec c1 ping c2		---> will work here as containers are connected to custom network having embedded dns. 

docker network connect bridge c1			--> we are connecting c1 to default network. we can connect to multiple networks. 
docker network disconnect bridge c1

Note:
		n1 (network)			n2
 	---------------			 -------------------
        | c1n1   c2n1 | --> containers	 | c1n2    c2n2    |
        ---------------                  -------------------
Here if c1n1 wants to connect to c1n2 then we should connect 1. n2 to c1n1 or n1 to c1n2  2. create new network n3 and attach c1n1 and c1n2 to n3

Host network: container uses host IP and port number. we cannot have multiple containers running with same port.
pros: network performance (low latency required then) + easy network troubleshoot + legacy system need host configuration 
cons: container ports are exposed to host directly causing sec vul + removes network isolation can be security risk
ex: docker run -d --name c1 --net host nginx: latest

none network: containers will only have local loopback address. cotainers kept in total isolation. when want to disable networking functionality for specific container. 
ex: docker run -d --name c1 --net none ngnix: latest


-----Load balancing in docker------
Process of distributing traffic load to group of backend servers(server pool)

reverse proxy: 
1. hides topological design of backend servers and even in attack takes max hit to protect app servers. 
2. act as cache to reduce load on backend servers.
3. easy to log and audit all requests. 

---load balancing through dns resolution(--net-alias)
docker run -d --net my_net --name c1 shantayyaswami/hello-flask:v1 --net-alias web
docker run -d --net my_net --name c2 shantayyaswami/hello-flask:v1 --net-alias web
docker run -d --net my_net --name c3 shantayyaswami/hello-flask:v1 --net-alias web

doker run -d --net my_net -p 80:80 --name nginx shantayyaswami/nginx --net-alias web

Note: --net-alias is used to provide common name to multiple container in network so that all containers can be accessed over network to do dns resultion. 
--links  --> used to connect 2 containers in defaut bridge network (deprecated option)
docker run --name c1 --links c2:c2		--> --links alias:container_name

---configure nginx as load balancer:
vi nginx.conf
http--> server--> 
upstream notes<lb name>{			--> add this directory
	server web:5000				--> c1,c2,c3 container application running on port 5000
 }	
server{
 listen 80;
location /{
 # proxy_pass <destination lb name>
   proxy_pass http://notes;
}
}


-------------------------------------Docker compose: (containers created though docker compose shares the same network)-----------------------------------
Tool that allows you to run multi-container application as single service. defined using declarative format in single node(YAML file).
Note:compose does not use swarm mode to deploy services to multiple nodes in swarm. To deploy apps across swarm use
"docker stack deploy"
define docker-compose project:
1. make dir to contain your docker-compose proj
2. change to proj dir
3. add docker-compose.yaml to dir
4. define apps using docker-compose.yaml
ex: 
docker-compose file structure:
version:
services:
  container-01:					--> we can also build images using build keyword instead of using already built images. 
    build: 
      context: <location of dockerfile>
      dockerfile: Dockerfile.prod		--> no need to use dockerfile keyword if name is Dockerfile
    image: <if we use buid then this keyword will tag to image build in above step>  
    env:
    ports:
    volumes:
    networks:
    restart: 
  container-02:
    command: 					--> its like CMD we mention in dockerfile or running container. 
    image: 
    env:
    port:
    volumes:
    networks:
volumes: <optinal>				--> whenever keyword appended with 's indicates list
 - my_db
networks: <optional>
 - my_net


Note: volumes and networks are same as 
	docker create volume my_db
	docker create network my_net
by default docker compose creates custom network for you. use depends_on: <list> to control startup service order under container to use dependency management. 
-----------------------
version: '3'
services: 
   web: 
     image: nginx
     port: 
     - '8080:80'
   redis: 
     image: redis:alpine

version: '3'
services:
  mongodb: 
    image: mongo
    ports: 
     - "27017:27017"
    environment:
     - USERNAME: admin
     - PASS: password
    volumes: 
     mymongo-data: /data/db
  mongo-express:
    image: mongo-express
    ports: 
     - "27017:27017"
    environment:
     - USERNAME: admin  
     - PASS: password
     - mongo-server: mongodb
volumes: 
  mymongo-data: 
     driver: local 

docker-compose up -f <filename>
docker-compose down -f <filename>

---create and run resources
docker-compose up -d 
docker-compose ps     --> list containers/services running under docker-compose
docker-compose down   --> stop and remove all resources

Docker stack:
A Stack is a collection of interrelated services that can be deployed and scaled as unit. docker stack are similar to the multi container apps created using docker-compose. However they can be scaled and executed across swarm cluster.
docker stack deploy -c COMPOSE_FILE STACKNAME    --> Deploy stack to swarm cluster using compose file
docker stack ls			--> list current stacks
dockr stack ps STACKNAME     --> list the tasks associated with stack
docker stack services STACKNAME   --> list services associated with stack
docker stack rm STACKNAME   -> delete stack


docker stack use the same compose file with addition of below tags under container. 
deploy:
 replicas: number


---customize docker compose files for different env---
Customizing a Docker Compose file for different environments is a common practice to adapt the configuration settings based on where you are running your application (dev/test/prod).

#docker-compose.yml (Main Compose File)
version: '3'
services:
  web:
    image: my-web-app:latest
    ports:
      - "80:80"
    environment:
      - DATABASE_URL=mysql://user:password@db:3306/mydatabase


#docker-compose.dev.yml (Development Environment)
version: '3'
services:
  web:
    ports:
      - "8080:80"
    environment:
      - DATABASE_URL=mysql://devuser:devpassword@dev-db:3306/devdatabase

docker-compose -f docker-compose.yml -f docker-compose.dev.yml config

#output
services:
  web:
    environment:
      - DATABASE_URL=mysql://devuser:devpassword@dev-db:3306/devdatabase
    image: my-web-app:latest
    ports:
    - 8080:80
version: '3'

Note: The priority is given to the settings in the last specified file, so settings in docker-compose.dev.yml take precedence over those in docker-compose.yml

------------------------------Container orchestration-------------------
Container Orchestration: all about managing lifecycle of containers(starting,stopping,moving containers across diff hosts,load balancing,HA etc)
Docker Swarm: feature of docker, which allows you to build distributed cluster of docker machines to run containers.
it facilitates orchestration, high availability and scaling. 

				docker CLI
				   ↓
			 ---------------------------------
 			|	swarm manager 		 |
			| 				 |
	store()	<-------|--scheduler	Discovery service|
			----------------------------------
			API over↓ http		↓
			--------↓------  -------↓---------
			|docer daemon |  | docker daemon |
			|swarm worker |  |swarm worker   |
			|-------------|  |---------------|

manager: manages cluster
worker: run the tasks assigned by scheduler
scheduler: schedule container onto nodes based upon rules and filters
Discovery service: helps swarm manager discover new nodes and fetch their details 
store(key-value db): stores state of cluster. 

overlay-network: Overay network spans across entire cluster allowing communication between containers across multiple nodes. (default for swarm)
docker create network my_net --driver=overlay

Note: following ports must be open for swam cluster to work
TCP port 2377 for cluster management communicaton
TCP and UDP port 7946 for communication among nodes
UDP port of 4789 for overlay network traffic

docker Swarm		 vs			Kubernates
1. easy installation 				complex installation process
2. manual scaling				supports auto-scaling
3. auto load-balancing				setup manual load-balancer
4. supports third party monitoring tools	Built in monitoring 
5. Integrates docker CLI			need for separate CLI (kubectl)

configuring swarm manager:
1. install docker-ce on swarm manager machine
2. initialize docker swarm with <docker swarm init --advertise-addr private-ip>
3. check <docker info>
4. list current nodes in docker swarm <docker node ls>

configure swarm nodes:
1. install docker-ce on worker node
2. get join command from swarm manager
--> run <docker swarm join-token worker> on manager to get join command
3. run join command on worker nodes provided in step 2
4. verify worker node joined manager or not with <docker node ls> on manager node
 
Backup and restore docker swarm:
1. stop docker service
2. take backup of /var/lib/docker/swarm dir
3. start service


Lock and Unlock swarm cluster: 
docker swarm encrypts sensitive data for security reason (logs on swarm manager + TLS communication btween swarm nodes) 
swarm manages the keys(unenrypted) used for above encryption so Autolock is the feature which locks the swarm.
this gives greater control of keys and can allow greater security. However, it require to unlock swarm every time swarm manager restarts.
<docker swarm unlock>
docker swarm unlock-key   -> if forget key
docker swarm unlock-key --rotate

Enable and disable Autolock:
docker swarm update --autolock=true
docker swarm update --autolock=false

High availability in swarm cluster:
Multiple managers  -> good to have to maintain high availability and fault tolerent swarm. Docker uses Raft consensus algo
to maintain consistent cluster state across multiple managers.
Quorum -> is majority(>half) of managers in swarm. It must be maintained to make changes in swarm cluster state.
Docker recc that you distribute your manager nodes across at least 3 AZ.

Services:
A service is the image for microservices like HTTP server,db server,any type of executable program that you wish to run in distributed env. A service specifies a set of one or more replica tasks. These tasks will be auto distributed across nodes in cluster and exec as containers.

Docker service create [OPTIONS] IMAGE ARG
ex:  docker service create --name nginx --replica 3 -p 8080:80
--replica   			-> number of replica tasks to create for service.
--name     			--> name of the service
-p PUBLISHED_PORT:SERVICE_PORT   --> publish a port so that service can be accessed externally

Templates can be used to give dynamic values to some flags with docker service create
--hostname
--mount
--env
ex: docker service create -env NODE_HOSTNAME="{{.Node.Hostname}}" --replica=3 --name web nginx
docker service create --constraint=node.role=manager --replica=3 --name web nginx			--> use constraint to run containers on specific node

docker service ls     -> list current services
docker service ps SERVICE   -> list service tasks
docker service inspect SERVICE   --> get more info about service
docker service update [OPTION] SERVICE   --> make changes to service
docker service rm SERVICE --> delete an existing service

replicated     vs    Global services
replicated services will run req number of replica tasks across swarm cluster.
ex: docker service create --name webserver --replica 3 nginx
global service will run one task on each node.
ex: docker service create --mode global nginx

scaling services: there are 2 ways to scale services
1. using docker service update
ex: docker service update --replica <REPLICAS> SERVICE --> docker service update --replica 2 nginx
2. using docker service scale
ex: docker service scale SERVICE=REPLICAS   --> docker service scale nginx=2
docker service update --rollback <service name>		--> to rollback to previous state

Node Labels: 
You can add peices of metadata to your swarm nodes using node labels. Then you can use labels to determine which nodes
tasks will run on. 
Syntax: docker node update --label-add LABEL=VALUE NODE       --> add label to nodes
docker node update --label-add availability_zone=east node1
docker node update --label-add availability_zone=west node2

docker node update --availability drain <node_id>			--> for maintainance
docker node update --availability active <node_id>

docker node promote		--> promote as manager
docker node demote		--> demote one or more nodes from manager

docker swarm leave		--> to leave cluster

Node constraints: To run services tasks only on nodes with specific label value use --contraint flag with docker service create
ex: docker service create --contraint node.labels.LABEL=VALUE IMAGE
docker service create --constraint node.labels.availability_zone=east nginx   or
docker service create --constraint node.labels.availability_zone!=east nginx 

Placement-pref: use --placement-pref with the spread strategy to spread tasks evenly across all values of perticular label.
docker service create --placement-pref spread=node.labels.LABEL IMAGE
ex: docker service create --placement-pref spread=node.labels.availability_zone nginx

----------------------------------------------------------------------------------------------------------------------------------------- -------
offcial docker images: dont have / in names + have official sign and name as OFFICIAL IMAGE

--configure non-ssl private registries
1. open /etc/docker/daemon.json file
2. edit file wth below info
{
"insecure-regustries":["http://private_registry_url"]
"data-root":location					--> use this to update new docker home
}
3. restart docker daemon

ONBUILD: it sets up trigger that gets executed when image is used as base for another image. it used to automate actions in child image when current image is used as base. 
ONBUILD COPY . /app					--> used very less

Docker best practices:
1. Use the docker official images as base image
2. Use Specific base image version (Latest images are tent to be unstable)
3. Use small-sized official images  
4. Optimize the caching image layer (order docker directives to use caching)
5. use .dockerignore to exlude files and folders. 
6. make use of multi-stage builds (for smaller size of images)
7. Use least privelaged user.   (docker has root permision which can lead to exploitation of app hosted inside container)
8. Scan images for vulnerabilities. 


challenges faced in docker:
1. docker is single daemon process can cause single point of failure. (podman solution)
2. docker daemon runs as root user making it vulnerable If the daemon is compromised, it potentially provides access to the entire host system.
3. docker does not provide inbuit credential management mechanism we need to depend upon third party integration 

what is docker?
"Docker is an open-source containerization platform for developing, packaging, and running applications consistently across various environments. In simple terms, it allows a process to run in a standalone environment with its dedicated space and resources, achieved through the use of namespaces and cgroups, which are features of the operating system."


How VM are different from containers?
1. Resource efficiency: light weight/heavier
2. speed and performance: start quickly/taks long time
3. islation: use kernal feature like cgroups and namespaces /stronger isolation with its own guest os
4. portability: solve portability issues/can have portability challenges due to diff os 
5. size: small/large


what is docker lifecycle?
1. User Interaction:
The Docker lifecycle typically begins with user interaction through the Docker client, which communicates with the Docker daemon.
2. Connection to Docker Daemon:
The Docker client connects to the Docker daemon using  /var/run/docker.sock. 
3. Image Check in Local Cache:
Before creating a container, 
4. Image Pull (if not found locally):
 /etc/docker/daemon.json, for authentication details to access private repositories.
5. Container Creation:
Docker daemon creates a container based on that image. This involves setting up namespaces, cgroups, and other isolation mechanisms to ensure the container runs independently.
Container Lifecycle:
1. Created: The container is created but not yet started.
2. Running: The container is actively running.
3. Exited: The container has completed its execution or has been manually stopped. This can include a "succeeded" phase if the container completed its task successfully.

what are different docker components?
1. docker cli
2. docker daemon
3. docker registry
4. docker image
5. docker containers
6. docker volume
7. docker network
8. docker compose


differce between ADD/COPY, ENTRYPOINT/CMD,ARG/ENV?
COPY ->  copy file from host to image
ADD -> similar to COPY but also pull files using URL and extract archive into loose file in images

CMD  -> sets default commands and/or parameters which can be overridden when the container is run. can have multiple CMD but last one referred.
ENTRYPOINT --> specifies the main command that will be executed when the container starts. CMD just pass a argument/parameter to an entrypoint. 
ENTRYPOINT + CMD if given both in dockerfile. use --entrypoint=<cmd> to replace entrypoint. only single entrypoint allowed. 

ARG  -> temp variable used while building an image. Post img built, the values of ARG are no longer available or accessible by processes running inside the container.
use --build-arg option to override it while building image.
ENV  ->  set env variables which is used at runtime(container). use -e option to override it while launging container.


what are different types of docker network?
1. Bridge Network (Default):
Description: This is the default network mode in Docker. It allows containers to communicate with each other using a virtual Ethernet bridge. Each container connected to the bridge has its own unique IP address.
Communication: Containers can talk to each other using these IP addresses. Docker also provides a DNS resolver for easy communication between containers using their names.
Port Mapping: Many container ports can be mapped to a range of host ports.

2. Host Network:
Description: In this mode, containers share the network namespace with the Docker host. They use the host machine's network directly.
Communication: Containers essentially behave as if they are running on the host machine, using its network configuration.
Port Mapping: Port mapping is straightforward, with a one-to-one mapping of container ports to host ports.

3. Overlay Network:
Description: The overlay network is primarily used in the context of container orchestration systems. like Docker Swarm mode. Containers in the swarm can communicate seamlessly across different hosts.
Communication: Overlay networks enable communication between containers running on different machines within the Swarm cluster.
Port Mapping: Similar to the bridge network, many container ports can be mapped to a range of host ports.

4. MacVlan:
Description: MacVlan (MAC Virtual LAN) allows you to assign a MAC address to each container, making them appear as individual devices on the network.
Communication: Containers in a MacVlan network can communicate directly with other devices on the network using their assigned MAC addresses.
Port Mapping: Port mapping in MacVlan is similar to the host network, with a one-to-one mapping of container ports to host ports.



what is multi-stage build?
Helps to build docker container in multiple stages allowiing us to copy artifacts from one stage to other making containers light weight. 



what steps you would take to secure containers?
1. Use distroless images (Regularly update base images to patch known vulnerabilities. Even distroless images may need updates.)
2. Implement network isolation (If container networking is not properly configured, containers may have overly permissive network access, potentially exposing sensitive services to unauthorized entities.) 
3. use image vul scanning usitilies like Trivy,anchore engine etc ( Integrate vulnerability scanning into your CI/CD pipeline to catch issues early in the development process. Regularly scan and update images in production.)
4. Implement the principle of least privilege. run containers with non-root user.
5. Limit resources (--cpu, --memory)
6. regularly update soft (os and other soft security patches)
7. Enable content trust and image signing (Docker Content Trust ensures the integrity and authenticity of container images.)
/etc/docker/daemon.json
{
  "content-trust": true
  "user": "non-root-user"
}
8. Use Docker Bench Security periodically:  (its a script that checks for dozens of common best-practices around deploying Docker containers in production.  assess the security of your Docker daemon configuration.)
git clone https://github.com/docker/docker-bench-security.git
cd docker-bench-security
sudo sh docker-bench-security.sh
review results --> The checks are categorized into sections such as 
"Host Configuration," 
[INFO] 1 - Host configuration should be backed up
[PASS] 2 - Ensure /etc/docker directory permissions are set to 755 or more restrictive
[PASS] 3 - Ensure /etc/default/docker file permissions are set to 644 or more restrictive
[WARN] 4 - Ensure that the daemon.json file permissions are set to 644 or more restrictive

"Docker Daemon Configuration,"
[INFO] 1 - Restrict network traffic between containers
[PASS] 2 - Set the logging level
[PASS] 3 - Allow Docker to make changes to iptables
[INFO] 4 - Disable the experimental `dockerd` features
[FAIL] 5 - Do not use the aufs storage driver


"Container Images and Build File." 
[INFO] 1 - Restrict network traffic between containers
[PASS] 2 - Set the logging level
[PASS] 3 - Allow Docker to make changes to iptables
[INFO] 4 - Disable the experimental `dockerd` features
[FAIL] 5 - Do not use the aufs storage driver


Note: By default, processes in a Docker container run as the root user within the container's user namespace. use non root user to run containers too.


how to run docker daemon as non-root user?
By default, the Docker daemon runs with root privileges. To enhance security, consider running the Docker daemon as a non-root user.
/etc/docker/daemon.json
{
  "user": "non-root-user"
}
Ensure that the specified user has the necessary permissions and is a member of the docker group.


how to run containers as non-root user?
1. Specify User During Container Run:
docker run -u or --user <UID>:<GID> your-image

2. Use a Named User:
docker run --user username your-image			--> instead of uid and gid use name directly

3. Set User in Dockerfile:
FROM your-base-image
RUN useradd -m -s /bin/bash myuser			--> creat nn-root user
USER myuser						--> specify default user

4. Use User Namespace:
sudo unshare --user						--> Create and enter the user namespace
useradd -m user1						--> Create a non-root user 'user1' in the first namespace
exit								--> Exit the first user namespace
sudo docker run -it --rm --userns=user1 ubuntu			--> Run a Docker container in the first user namespace

5. Specify User in Docker Compose:
services:
  your-service:
    image: your-image
    user: "1000:1000"

6. Runtime Configuration (for Kubernetes):			--> use securityContext
apiVersion: v1
kind: Pod
metadata:
  name: your-pod
spec:
  containers:
  - name: your-container
    image: your-image
    securityContext:
      runAsUser: 1000
