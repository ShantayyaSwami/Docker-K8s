kubectl version --short
Skooner: K8s dashboard
Container orchestration automates the deployment,management,scaling and networkng of containers across the cluster. it focus on managing the lifecycle of containers. 
K8s  -> open-source platform for managing containarized workload and services. 

Vertical scaling: To scale more, add one or more ram,cpu,memory to the existing machine.
Horizontal scaling: To scale more, add more machines to the existing group of distributed systems.  

K8s objects:
compute: pod,node,namespace,deployment,Replicaset,statefulset,Daemonset
networking: service,ingress
storage: persistentVolume, StorageClass, PersistentVolumeClaim

K8s Architechture overview:1.24

Control plane           	kubectl                              Worker Nodes(30000-32767)                   
-----------------------------------↓-----------        --------------------------------------------------------
|________________           _______↓_________ |       |------->Kube Proxy Server|------>Kube Proxy Server    |
|Kube Controller|---------> |Kube API Server|<------->|                         |                            |
|____mgr_10252__|           |___6443________| |       |------->Kubelet 10250    |------>Kubectl              |
|                           ↑     |           |       |          ↓              |         ↓                  |
| Kube Scheduler (10251)----|     |           |       |     Container Runtime   |    Container Runtime       |                           
|                           ↑     ↓           |       |     (docker/containerd) |     (docker/containerd)    |
| Cloud Controller mgr------↑ETCDCTL(2379-80) |       |                         |                            |
|_____________________________________________|       |_________________________|____________________________|

kubectl: command line utility used to interact with k8s cluster. communicates with API server through REST API calls for CRUD workloads in kubernetes. 

Control plane: 
is collection of multi components responsible for managing/controling K8s cluster globally. 
Individual control plane components can run on any machine in the cluster. But usually run on dedicated controller machines.

Kube-API-Server --> serves K8s API. primary interface to control plane and cluster itself. When interacting with K8s cluster we do so using K8s API.
ETCD --> ETCD is key-value db, stores the state of K8s cluster. X store app or db data
Kube-Scheduler --> handles scheduling of pods, irrespective of whether pods are running or stopped. 
Kube-Controller-Manager --> brain behind orchestration. It does controller utilities like detecting failed pods and restarting them.(manage desired and current state)
Cloud controller manager --> provides an interface between K8s and cloud platform. It is only used when using cloud based resources with K8s. 


K8s Nodes: 
are machines where containers managed by the cluster run. cluster can have any number of  nodes. 
Various node components manage containers on machine & communicate with control plane.

Kubelet --> k8s agent runs on each node. It communicate with control plane(API server) about state of node and ensures that containers(through CRE i.e containerd or docker) run on its node as instructed by control plane. 
Kubelets also handles the process of reporting container status and other data about container back to API server. 
Container Runtime --> is not build into k8s. Its a separate peice of software that is responsible to run container on each machine. K8s supports docker/containerd etc
Kube-Proxy --> is network proxy. It runs on each node and provides networking between containers and services(single entrypoint which manages backend nodes) in the cluster by maintaining iptable.  

CNI: container networking interface maintains the routing rules for container communication inside cluster. 
CRI: container runtime interface acts as bridge between kubelets and container runtime. 
coreDNS: DNS service which resolves names to IP.
service: single entrypoint which manages backend nodes. pod to pod communication is very difficult without service as containers are not permanent. name and IP gets change if new container gets created.  so container can talk to service (virtual object. its not container rather configuraton) which manages the backend nodes(pods). 


-----Journey of kubectl request------

kubectl 	   kubectl				 kubectl					kube API server
 | 		  Client validation	--------------> form the HTTP request	-------------------->  server side validation		----------------> ETCD	
 ---------------> 1. check YAML syntax			1. forms API calls				1. performs authentication      --------------> controller 
 kubectl run	  2. misconfiguration			2. authentication and cert validation		 and authorization		 -------------> Scheduler
 kubectl apply	  3. non-supported resources		for user validation				2. verifies the API		-------------> kubelet +
		  4. fail fast mechanism to avoid	3. reads kubeconfig file									container
 		loading API server			4. submit pod spec to API server


1. kubectl forms API request and send it to the API server.
2. APi server authenticates and authorise
3. API server writes pod object to etcd data store. once write is successful, aknowladgement is sent to API server and to the client.
4. now pod is in pending state
5. Scheduler sees that new pod object is created but not bound to any node
6. Scheduler assigns node to the pod and udpates API server.
7. API server updates this to the etcd data store. 
8. kubelets keep querying the API server for any new work loads. it sees that new pod is assigned to it. 
9. kubelets instructs the container runtime like docker to create container and update the container state back to API server.
10. APi server update the pod state as running in etcd data store. 

kubectl	 <---------> API server	<--->	etcd			scheduler		kubelet		docker
			<-------------------------------------->
			<------------------------------------------------------------->
											 <-------------->	

pod : smallest deployable unit that holds and runs the container(one or more).
 
Kubernates cluster:
kubeadm is a tool that will simplify the process of setting of K8s cluster. 
install containerd/docker --> install K8s packages (kubeadm,kubelet,kubectl) --> initialize cluster with <sudo kubeadm init --pod-network-cidr range --kubernates-version 1.24.0
--> setup .kube config command from output for cluster communication --> execute join command on nodes from output --> deploy calico network plugin 

---------kubectl syntax------
kubectl 	command 	type		name		flag
		(create		pods or po	optional	-f or --filename
		 get		deployments or 			-o or --output
		 delete		deploy
		 attach		namespaces(ns)
		 apply		
		 describe
		 autoscale)

kubectl get namespaces   ---list namespaces
kubectl get pods --namespace or -n my-namepsace  --list pods from my-namespace if namespace name not given cmd will fetch pods from default namespace
kubectl get pods --all-namespaces
kubectl get pods -o wide --> get detailed info with PODS IP address and node name
kubectl get nodes --> list cluster nodes


High Availability in K8s:
K8s facilitates high availability apps but you can also design the cluster itself to be highly available. To do this you need multiple control plane nodes.

Control plane node1 ....control plane node2
kube-api-server		 Kube-api-server									
             ↑            ↑
              Load balancer
                 ↑
Worker node1     ↑
kubelets    -----↑

1. Stacked ETCD   ---> ETCD runs on same control plane node
2. External ETCD  ---> ETCD runs on separate machine but not on control plane node

K8s Management tool: 
There are variety of K8s management tools available. These tools interface with Kubernates to provide additonal functionality
Kubectl --> Official CLI for K8s. it is the main method you will work with K8s.
kubeadm --> is a tool that will simplify the process of setting of K8s cluster. 
Minikube --> allows you to automatically set up a local single-node k8s cluster. Its great for k8s dev purposes. 
Helm --> provides templating and package management for K8s objects. You can use it to manage your own templates (known as charts). you can download and use shared templates.
Kompose --> helps you translate docker compose files into K8s objects. If you use docker compose for some of your workflow then can move apps to K8s using Kompose.
Kustomize --> configuration management tool for managing K8s objects configurations.It allows you to share and re-use template configurations for K8s apps.


kubeconfig: file used by kubectl to retrieve the required configuration of kubernetes cluster or to communicate with API server of kubernetes cluster. 
by default, kubectl looks in below locations
1. $HOME/.kube dir
2. export KUBECONFIG=$KUBECONFIG:$HOME/.kube/config  			--> env variable
3. kubectl config --kubeconfig=config-demo view --minify		--> command line flag

kubeconfig file: user -->context,namespace --> cluster			--> context does the mapping of user to cluster 

context: refers to set of access parameters(cluster server,namespace,user) for kubernetes cluster. used when working with >1 clusters and have diff configurations.
kubectl config get-contexts/current-context/use-context <context-name> : Display available contexts/current context/Switch to a different context

---------------------------Pods-------------------------

Pod: smallest deployable unit that holds and runs the container(one or more). managed by controler manager. each pod has name and IP. containers within same pod shares same n/w,volume and namespace. pod spec is the YAML file that k8s use to describe about containers inside pod. 
pod lifecycle
1. pending: containers not created yet. in the state of downloading container image over n/w
2. running: pod is scheduled on node and atleast one container running state.
3. succeeded: pod exited with status 0 and will not restart
4. Failed: all containers in pod exited but atleast one with non-zero status
5. CrashLoopBackOff: container fail to start and tried again and again (Application issues,Resource constraints,missing dependancies(cm),incorrect image or version, configuration issues (env var,mounts etc),Health check failure(probes), startup delays, permission issues etc
6. Unknown: for some reason state of pod could not be obtained. 

container lifecycle:(waiting,running,terminated) 
pod restart policy:
  1. Always  --> default restart policy in K8s. containers always be restarted if they stop, or completed successfully
  2. On-Failure --> Will restart container only if container process exits with error code or containr detect to be unhealthy by liveness probe. X after successfully completed. 
  3. Never  --> never be restarted. even if container exits or liveness probe fails. 

multi-container pod design patterns:
It is not adviced to burden the main application with additinal responsibilties like logging,monitoring,conneting to dB etc.keeping the image as small as possible to reduce attack surface. 
1. Init Containers:Init containers are exactly like other containers except they always run to completion. init containers must complete successfully before next one starts. if pods init container fails K8s repeatadly restarts the pod untill init container succeeds. if restart policy set to Never then K8s treat overall pod as failed. you can use init containers to do variety of startup tasks. They are often useful in keeping main containers lighter and more secure by offloading startup tasks to separate container. (works like bootstrap script)

apiVersion: v1
kind: Pod
metadata:
  name: init-container-pod
spec:
  initContainers:
  - name: init
    image: busybox:latest
    command: ["/bin/sh", "-c"]
    args: ["i=10; while [ $i -ge 0 ]; do echo $i; sleep 0.5; i=$(expr $i - 1); done"]
  containers:
  - name: main-container
    image: busybox:latest
    command: ["/bin/sh", "-c"]
    args: ["echo 'Hello from main container'"]



2. Sidecar: sidecar pattern uses the helper container to enhance or extend the functionality of main container. it does additional functionalities like logging,monitoring,security etc. failures in sidecar will not impact main container.
apiVersion: v1
kind: Pod
metadata:
  name: sidecar-pod
spec:
  volumes:
  - name: temp
    emptyDir: {}

  containers:
  - name: side-car
    image: busybox:latest
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo 'This is from $(hostname)' >> /var/log/index.html; sleep 1; done"]
    volumeMounts:
    - name: temp
      mountPath: /var/log

  - name: application
    image: nginx
    ports:
    - containerPort: 80
    volumeMounts: 
    - name: temp
      mountPath: /usr/share/nginx/html

3. Ambassador: proxy pattern. acts as gateway or mediator for n/w communication. often handling tasks such as routing,load balancing etc. handles requests/responses from/ to remote services or external services. 

4. Adaptor: serves as translator between diff components. plays crutial role in enabling interoperability between diverse tech or services. (Log format changer)

image pull policy:
determine if the container image should be pulled from repo prior starting container. 
value		description
always		always pull	if tag is latest
ifNotPresent	if not exist	(default)
never		never pull

ImagePullBackOff: status condition occur while pulling image (Authen issue,n/w issue, image not availble,registry rate limit,registry not available etc)
		To avoid above issue due to auth we can use imagepullsecrets at container level. 

5. pause container: its design pattern used to share the n/w namespace among container in pod for internal container communication plus ensure that IP address remains constant when container restart. 

imparative/declarative commands: 
imparative: command line argument
	kubectl run web --image=nginx:latest	--dry-run=client			--> shows what API object is being created at client side
	kubectl get po -o wide/json/yaml						--> shows deployed pods
	kubectl run web --image=nginx --dry-run=client -o yaml >pod.yaml		--> gives you manifest file
	kubectl run web --image=nginx --label="app=hazelcast,env=prod"	--> label is key value pair
	kubectl logs -f nginx						--> show pod logs. append -c containername -l key=value(label)	--tail=20 --since=1h
	kubectl delete po nginx	--force --grace-period=0		--> want to force kill. grace period is 30sec default
	kubectl edit po nginx						--> can update pod manifest file
	kubectl exec -it nginx -- command				--> exec command inside pod. if pod has >1 container then use -c containername. -- 										differentiates kubectl commands with actual sh command. 
	kunectl run curl --image=curlimages/curl -i --rm --restart=never -- curl IP		--> use --rm for temporary pods. 

	kubectl logs/describe (events)/events -n namespace_name		--> used for debugging
	kubectl describe pod <pod_name>
	kubectl get events --field-selector involvedObject.name=pod_name
	kubectl get pod <pod_name> -o jsonpath='{.spec.containers[*].name}'		--> get container names from pods. 

decalarative: describing k8s objects in yaml file
kubectl create -f podfile.yml				--> create prod. if already exist then throws error
kubectl apply  -f podfile.yml				--> create + update pod	


Manifest/Spec file:   describe k8s objects in yaml file
Manifest file has 4 mandatory fields:
	1. apiVersion		--> kubectl api-resources 
	2. kind			--> type of resource(deployment,pod,service)
	3. metadata	
	4. spec			--> includes replicas,selector,template(spec--> containers) for pod etc
            default-container: <container name> 				--> if pod has multiple containers

-----man pages--------------
# list all K8s API supported objects
kubectl api-resources
kubectl explain pod

Note: command(ENTRYPOINT) and CMD(arg) in kubenertes. use --command -- <arg> 
ex: --command ping -- 8.8.8.8

Ephemeral containers: additional container(with debug utilities like bash/sh/curl etc) in pod for debug purpose.  Not restarted when exit or pod removed or restarted.
	|________> Application container(distroless image)

kubectl debug -it ephemeral-demo<pod name> --image=busybox:1.2 --target=ephemeral-demo		--> create debug container in ephemeral-demo pod

Note: When using distroless images for your main application container in Kubernetes, troubleshooting can be a bit challenging
1. sidecar container
2. Temporary debug container
# Add the debugging container
kubectl edit pod <pod-name>					--> use rolling update method

# Troubleshoot using kubectl exec
kubectl exec -it <pod-name> -c debug-container -- /bin/sh

# After troubleshooting, revert the pod
kubectl apply -f original-pod-spec.yaml
3. log analysis
kubectl logs <pod-name> -c main-container


----Labels,annotations and Selectors:
Labels are key-value pairs attached to the K8s objects like pod. run = <object name> is deafult label
kubect get po --show-labels
kubectl label po <podname> env=prod tier=frontend		--> add label to running containers. --overrite if want change existing one. 
kubectl label po <podname> env-					--> use key- to remove label

labels allows you to identify,select,operate on k8s objects. where as annotations are non-identifying metadata. Annotations can hold any kind of info which can be useful and provide context to DevOps teams. 

kubectl annotate po <podname> key=value

selectors: used to filter K8s objects based on labels. 
1. equality based selectors (=, !=)		--> used by Replication Controllers, services
kubectl get po --selector env=prod
2. set-based selectors	(in,notin,exists())	--> filter keys acccording to set of values. used by ReplicaSets, Deployments,DaemonSets. 
kubectl get po --selector env in (prod,qa)	


--------------------Replica Sets and Replica Controllers-------------(Reliability,Load balancing and scaling)
ReplicationController:  is deprecated and has been replaced by ReplicaSets. they use equality based selectors. 
ReplicaSet:  is a lower-level controller that ensures a specified number of replica Pods are running based on desired and current state. use set based seletors. 

kubectl get rc/rs
kubectl delete -f replicaCntroller.yml
kubectl scale rc/rs nginx-rc/rs --replicas=10

Note: selector must be of same name as defined in pod metadata. otherwise throws error. 
ReplicaController.yml
apiVersion:v1
kind:ReplicaController / ReplicaSet
metadata:
  name: nginx-rc
spec:
  replicas:3
  selector:
   app: nginx
  template:
    metadata:
     labels:			matchLabels:			--> here it will be matchLabels(AND operated) which shows set based selector for ReplicaSet. 
      app: nginx 			app: nginx	           we can use matchExpressions:(OR operated) for multiple selectors
    spec								       - key: app
      container:								 operator: In
      - name: nginx								 values: [nginx,prod]
        image: nginx
        port:
        - containerPort: 80

Note: if we manually create pod with same label as above then ReplicaController/Set terminate manually created pod by thinking Desired and current pods number is same.  


-----------------Deployments and DeploymentController----------
Deployment:  is a higher-level controller that manages(internally creates) ReplicaSets and provides additional features like rolling updates,rollback,pause and resume update etc for deploying and updating applications.
Deployment is recommended way to deploy pod(deploymentname+rs-id+hash) or replicaSet. keep track of various deployed versions of apps for rollback. (not the case with ReplicaSet)

kubectl create deploy nginx --replica=5 --image=nginx:latest --port=80
kubectl get deploy
kubectl delete deployment nginx
kubectl rollout status deployment nginx
kubectl rollout history deplo nginx
kubectl rollout undo deploy nginx --to-revision=<get this revisionf rom above steps>
kubectl set image deploy nginx nginx:1.4	--> change image of container.
kubectl scale deplo nginx --replica=10  

rollout: its a process of gradually deploying or updating your application containers. 
----deployment strategies:
1. recreate: old version is terminated and new version is rolled out. easy but expects downtime
2. Rolling/Ramped update: default strategy. new version is slowly rolled out by replacing old version. default 25% for both
	1. maxSurge: number of pods created on top of existing ones. 
	2. maxUnvailable: number of pods down at a time. 
Note: if maxSurge is 0 then maxUnvailable can't be 0
3. Blue-Green: initially user traffic goes to Blue(current version) deployment, when new version deployment ready, route traffic to green env.
having two env and switching between them for updates. 
4. Canary: testing new version with small group before rollout. 
5. A/B testing: comparing two versions by exposing diff user to each version. 


---------------Namespace----------------------
K8s Namespaces: K8s objects such as pods and containers, live in namespaces. namespaces provide a way to partition cluster resources between multiple users, teams, or applications. You can use namespaces to create isolated environments. 
4 default namespaces:
1. default : as name says its default namespace when we create any k8s object
2. kube-system - user dont have access to it. k8s object such as kubectl, api-server are created under kube-system namespace.
3. kube-public - publically accessible k8s objects are created user kube-public like Congfigmap, secrete etc
4. kube-node-lease - contains hear-beats of nodes in k8s cluster + each node has associated least object in namespace + determine availability of nodes

every resource gets unique name which is combinatin of (resource name + namespace) called FQDN. fully qualified domain name. 

#create namespace: my-namespace.yml
apiVersion: v1
kind: namespace
metadata:
  name: my-app-namespace

#define resource quota: resource-quota.yml
apiVersion: v1
kind: ResourceQuota
metadata:
 name: my-app-quota
 namespace: my-app-namespace			--> bad practice to hard code namespace rather pass -n <namespace name> while deployment
spec:
 hard:
   pods:"number"
   requests.memory:					Mi --> Mebibytes (more than megabyte 10^6 its decimal unit sotrage but computer understand in binary unit which
   requests.cpu:					is 2^20 little more than megabyte					
   limit.cpu:						m --> millicore
   limit.memory:


kubectl get ns							--> list namespaces
kubetl delete ns dev						--> very careful while deleting ns. we can assign roles to limit actions 
kubectl deploy -f specfile.yml -n <namespace name>		--> create deployment in perticular namespace

curl pod-ip.ns.pod.cluster.local				--> to communicate with pod in diff ns using pod-dns


---------------------Services--------------------------
service: single entrypoint which manages backend nodes. pod to pod communication is very difficult without service as containers are not permanent. pod IP gets change if new container gets created. so container can talk to service (virtual object. its not container rather configuraton) which manages the backend nodes(pods).

IP's are stable and balance the load. service identifies its member pods with selector so pod must have labels specified in selector of service. 
types of service:
1. ClusterIP: default. assigns stable internal IP to set of pods enabling communication within cluster. ClusterIP services are often used for read-heavy operations.

apiVersion:
kind: Service
metadata:
spec:
  selector:
  ports:
  - protocol: 
    port: incoming port to service
    targetPort: outgoing port should be same as container port
  type: ClusterIP						--> if not specified then default is ClusterIP


kubectl expose deployment nginx-deployment --port=80 --target-port=80 --type=ClusterIP --selector=app=nginx		--> clusterIp is default so no need to mention
kubectl get ep web									--> show list of endpoinds managed by service

Note: Each pods use the information in /etc/resolv.conf to locate the DNS server, and CoreDNS(running in kube-system ns) is the DNS server responsible for resolving service names within the Kubernetes cluster.  kubeproxy maintains the IP table and share those with service. 

service FQDN: servicename.ns.svc(K8s obj).cluster.local
pods exposed by service gets dns like pod-ip.servicename.ns.svc.cluster.local

port-forwading: used for temp creating connection with pod for debug purpose. pods are not accessible directly from outside world without service expose. 
kubectl port-forward po/podname localport:podport
kubectl port-forward po/web 8080:80				--> forward data coming on local port 8080 to pod listening on port 80 

2. NodePort(no IP allocated rather use one of nodes IP): exposing service on static port on each node, allowing external access to service. 
port must be in the raneg of 30000-32767. its recommended to let K8s auto assign the port. NodePort uses the labels and selectors as ClusterIP

spec:
  selector:
  ports:
  - protocol: 
    port: incoming port to service
    targetPort: outgoing port should be same as container port
    nodePort: let K8s auto assign
  type: NodePort

drawback: exposing service through nodeport uses multiple nodes IPs and corresponding node port makes it difficult for end user to access apps makes it user unfriendly.
solution: Deploy HAProxy outside the cluster and configure it to balance traffic to the nodes using their NodePort.


3. LoadBalancer: is a way to expose services to the external world. When you create a service with type LoadBalancer, the cloud provider (if you're using a cloud-managed Kubernetes service) automatically provisions a load balancer and assigns it a public IP address. it internally creates nodeport. Load balancer accepted request on its static IP and forward request to nodeport on nodeport IP and port number.  

drawback: It can incur additional costs, especially in cloud environments where public IP addresses and load balancers are billable resources.
solution: Ingress controller with LB -> allow you to expose multiple services through a single load balancer and public IP address. 
load balancer sharing -> share a single load balancer among multiple services, reducing the number of provisioned IP addresses.


4. Headless Service(no IP allocated): (very imp for statefulset apps) It is used when you don't need a single, stable IP address for the service, and you want to directly reach individual pods backing the service(pod-pod communication).The service creates DNS records for each pod backing the service, allowing you to discover individual pod IPs using DNS. commonely used in database clusters, statefulsets etc
Use case: Headless services are often used for scenarios where direct communication with individual pods is necessary, such as in StatefulSets for databases.
Headless service still provide load balancing across pods through DNS round robin mechanism instead of service proxy.
Headless services are often used for write-heavy operations.

apiVersion:v1
kind: Service
metadata:
 name: my-headless-service
spec:
  ClusterIP: None
  ports:
  - protocol: tcp
    port: 80

Note: If we do nslookup on healess service FQDN then it resolves to backend pods Ip address.
nslookup my-headless-service.default.svc.cluster.local			--> gives backend pods IP's. each pod gets unique identity like podname.headless-servicename


External IP: The externalIPs feature is not primarily designed for managing external IP addresses of services, especially for external databases or external applications hosted outside the K8s cluster. Instead, it's used to assign specific external IP addresses to services within the Kubernetes cluster itself. but does not provide load balancing. 
spec:
  selector:
  ports:
  - protocol: 
    port: incoming port to service
    targetPort: outgoing port should be same as container port
    nodePort: let K8s auto assign
  type: NodePort
  externalIPs:
  - 203.0.103.1


Services without pods: selectors from service used to connect to endpoints. but service without selector does not create endpoints so we manually create endpoints (like aws rds Ip:port)
The Endpoints resource in K8s is used to associate a service with a set of network addresses. 

apiVersion: v1
kind: Service
metadata:
 name: headless-service
spec:
 ClusterIP: None
---
apiVersion: v1
kind: Endpoint
metadata:
 name: my-headless-endpoint
spec:
 - addresses:
    - 201.0.102.1						--> it can be external apps IP address which can be connected from K8s cluster. 
   ports:  
    - 5000

Note: when you create a Headless Service without selectors, you need to manage the Endpoints manually. The Endpoint object specifies the actual IP addresses and ports of the backend pods or external services that the Headless Service is responsible for.


ExternalName: it helps to connect to the service running outside of K8s cluster through mapping Service to dns name. IP address not allowed. 

apiVersion: V1
kind: Service
metadata:
 name: patment-gateway
Spec:
  type: ExternalName
  externalName: payment-gateway-api.external-provider.com


Ingress Controller: It is responsible for managing external access to services within the cluster. It allows you to define rules for routing external HTTP and HTTPS traffic to different services based on the host, path, or other criteria.it can be deployed through Helm charts, YAML manifests, or other deployment tools.
Make sure to deplo ingress controller before use ingress resource. 
1. Nginx Ingress
2. Traefik Ingress
3. HAProxy

---Ingress rules----
Ingress exposes HTTP and HTTPS  routes from outside the cluster to services within cluster. Traffic routing is controlled by rules defined in Ingress resource.
Ingress resource cannot do on its own. We need to have Ingress controller  in order for ingress resource to work through ingress class.  

Note: Reverse proxy and Ingress controller basic difference is ingress controller can dynamically reload configurations(without restart) which is not the case with Reverse proxy. 
 
----Nginx Ingress-----
It acts as a load balancer and handles the routing of incoming requests to different services within the cluster.

----------Traefik ingress--------
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-name
spec:
  ingressClassName: nginx  # Specify the Ingress controller type here (e.g., nginx, HAProxy, traefik)
  rules:
    - host: domain-name								# host/name based routing	
      http:
        paths:
          - path: /path1							# path based routing
            pathType: Prefix							# exact or prefix
            backend:
              service:
                name: path1-service-name  # Specify the service name
                port:
                  number: 80
          - path: /path2
            pathType: Prefix
            backend:
              service:
                name: path2-service  # Specify the service name
                port:
                  number: 80

Ex: Implementing sample real time Ingress controller.
Step1: install nginx ingress controller 

# Add NGINX Helm repository
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx

# Update Helm repositories
helm repo update

# Install NGINX Ingress Controller
helm install nginx-ingress ingress-nginx/ingress-nginx

step2: Deploy sample application
# app1-deployment.yaml		#create app2-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app1
spec:
  replicas: 2
  selector:
    matchLabels:
      app: app1
  template:
    metadata:
      labels:
        app: app1
    spec:
      containers:
      - name: app1
        image: nginx:alpine
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: app1-service
spec:
  selector:
    app: app1
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80

step3: Deploy ingress resource
# ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
spec:
  rules:
  - host: app1.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app1-service
            port:
              number: 80
  - host: app2.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app2-service
            port:
              number: 80

step4: # Apply the configuration. 
Apply the deployments
kubectl apply -f app1-deployment.yaml
kubectl apply -f app2-deployment.yaml

# Apply the Ingress resource
kubectl apply -f ingress.yaml


----------------------------------K8s Volumes------------------------------
volume is dir accessible to all containers running in pod. containers are ephemeral so to persist the data, K8s provide volumes. 
Ephemeral volumes: 
1. emptyDir(pod): temp storage created and managed by the lifecycle of pod. volume is empty on pod creation and volume gets deleted when pod stopped,rescheduled etc
apiVersion: v1
kind: Pod
metadata:
  name: example-pod
spec:
  containers:
  - name: example-container
    image: nginx:latest
    volumeMounts:
    - name: temp-volume					#same volume can be mount by multiple containers. 
      mountPath: /data
  volumes:
  - name: temp-volume
    emptyDir: {}


2. hostPath(node): The hostPath volume allows a pod to use a file or directory on the host machine as storage. if pod gets rescheduled to other node then directory or file gets created on that particular node from scratch. if same path exist then pod use the same. 
apiVersion: v1
kind: Pod
metadata:
  name: hostpath-pod
spec:
  containers:
  - name: container1
    image: nginx:latest
    volumeMounts:
    - name: hostpath-volume
      mountPath: /data
  volumes:
  - name: hostpath-volume
    hostPath:
      path: /path/on/host
      type: File or Directory or FileorCreate or DirectoryorCreate

----------------K8s Environment variables-----------
Env variables in K8s can be of key-value pairs, ConfigMaps or Secrets. Refer them using env or envFrom keys. 

#plain text
env:
 - name: DB_NAME
   value: "postgres"

kubectl run -it test --env="APP=test" --image=busybox-alpine --rm -- printenv

Note: to inject pod's data as containers env variable
env:
 - name: POD_NAME							--> use ["$(POD_NAME)"] to print env variable in containers in args
   valueFrom:
     fieldRef:
       fieldPath: metadata.name
 - name: POD_IP
   valueFrom:
     fieldRef:
       fieldPath: status.hostIP
         

---configMap--------
provide file as configuration. A configMap used to store tha non-confidential data in key-value pairs. pods can consume  ConfigMaps as 
1. Env variables
2. command line args
3. Configuration file in volumes. 

But if configuration have sensitive info then use secrets. configmap is namespaced obj. unlike env var, if these files(mounted as configmap) changes then new file pushed (eventualy consistent) to runnng pods without need of pod restart

kubectl create configmap (cm) app-config --from-literal=BACKEND_COLOR=blue
kubectl create cm app-config --from-file=./myconfig				--> here myconfig file name will be key and file content wil be value when use file

Note: cm has data field instead of spec in configMap manifest file. when we provide file as configuration then filename wil be key and its content will be value but its of no use so mount file as configMap and use it.
If we use configMap inside pod then configMap must be available otherwise pod will fail due to dependancy.  

apiVersion: v1
kind: ConfigMap
metadata:
  name: db_details
data:
  db_name: mysql
  db_host: mysql.wolrdpay.local
  db_env: prod
---
env:						--> using config map as env variable
  - name: DB_NAME
    valueFrom:
      configMapKeyRef:
        name: db_details
        key: db_name				--> for every key you have to repeat - name and valueFrom
envFrom:					--> if want to refer all variables from configmap
- configMapRef:
    name: db_detais


---to mount configMap file 
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
data:
  custom-nginx.conf: |
    # custom-nginx.conf
    server {
        listen 80;
        server_name localhost;

        location / {
            root /usr/share/nginx/html;
            index index.html;
        }
    }
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  containers:
  - name: nginx-container
    image: nginx:latest
    volumeMounts:
    - name: nginx-config-volume
      mountPath: /etc/nginx/conf.d/
  volumes:
  - name: nginx-config-volume
    configMap:
      name: nginx-config
      items:						--> used to rename file to new name
      - key: custom-nginx.conf						
        path: nginx.conf


-----Secrets--------
Secret is K8s obj used to store confidential data like pass,ssh key, tokens etc. same as configMap but specifically intended to hold confidential data.
Pods can consue secrets as
1. Env var
2. configuration file in volume
3. pull secrets by kubelet

Note: secrets are stored unencrypted (base64 encoded) in etcd. anyone with API access update it so use RABC to restrict users. while creating secrets through yaml put base64 values. 

secret types:
1. generic (Opaque)
2. tls
3. docker-registry

kubectl create secret generic my-opaque-secret --from-literal=username=my-username --from-literal=password=my-password
kubectl create secret tls tls-secret --cert=path/to/tls-certificate-file --key=path/to/tls-private-key-file
kubectl create secret docker-registry registry-secret --docker-server=my-registry.io --docker-username=my-username --docker-password=my-password 


apiVersion: v1
kind: Secret
metadata:
  name: db_credentials
data:
  DB_USER: base64 encoded value
  DB_PASS: base64 encoded value
---
env:
  - name: DB_USERNAME
    valueFrom:
      secretKeyRef:
        name: db_credentials
        key: DB_USER
or
envFrom:					--> use all keys from secret
- secretRef:
    name: db_credentials 

--------------------------------------Manual scheduling------------------
manual scheduling is sometimes needed  to ensure that certain pods only scheduled on nodes with specialized hardware like ssd or to co-locate services to communicate frequently (DB-apps pods to reduce latency)
K8s offer several ways to schedule pods
1. nodeName:  
simplest form but due to limitations not used. 
apiVersion: v1
kind: pod
metadata: 
  name: nginx
spec:
 containers:
 - name: nginx
   image: nginx:latest
   ports:
   - containerPort: 80
 nodeName: minikune-04

limitations: if node does not exist or node does not have enough resources or nodes in cloud are unpredictable 
 
2. nodeSelector:  (every node has labels + simplest recommended form)
its same as nodeName however node selector use labels to assign pod on node. limitation is if matching nodes are not available then pod stays in pending state.its sets hard rules. 

apiVerison: v1
kind: Pod
metadata:
  name: nginx
  lables:
    env: prod
spec:
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
  nodeSelector: 
     disk: ssd							--> kubectl get nodes -l disk=ssd

3. Node affinity and anti-affinity: nodeAffinity is conceptually similar to nodeSelector with few key enhancements
	1. nodeAffinity rules are more expressive. supports both soft/hard rules. 
	2. If scheduler can't find node with matching labels, pod still be scheduled on other 	nodes (soft rules).
	Supported Affinity rules: required --> indicates hard requirement that must be met	preferred --> indicates soft requir wch enforced but not guaranteed.
	requiredDuringSchedulingIgnoredDuringExecution
	requiredDuringSchedulingRequiredDuringExecution
	preferredDuringSchedulingIgnoredDuringExecution
	preferredDuringSchedulingRequiredDuringExecution   

apiVerison: v1
kind: Pod
metadata:
  name: nginx
  lables:
    env: prod
spec:
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
  affinity:
    nodeAffinity:
       preferredDuringSchedulingIgnoredDuringExecution:
       - weight: 1						--> 1-100 higher the weight, higher chances of scheduling pod on that node. if >1 affinity rules then
         preference:						based on weight one rule is selected	
            matchExpression:
            - key: disk
              operator: in
              value: 
              - ssd

Note: Anti-Affinity is opposite to affinity. anti-affinity looks for matching label and avoid scheduling pod on that node. 

4. Inter-pod affinity and anti-affinity

5. Taints(nodes)  and toleration(pods): Taints are opposite to nodeSelector. They allow a node to repel a set of pods. Taints are applied to nodes(lock). Tolerations are applied to pods(key). In Short, pod should tolerate the nodes taint in order to run in it. like having correct key with pod to unlock node and enter it. 

By default master nodes are tainted, so you can deploy .
kubectl describe node node-name | grep -i taint			--> to check taints applied on node. 
kubectl taint node node-name key=value:taint-effect		--> add taint to the node
kubectl taint node node-name key=value:tant-effect-		--> to remove taint

taint effetcs:
1. NoSchedule: hard requirement. unless pod has matching toleration, will not be schedule on node.  
2. preferNoSchedule: soft version of NoSchedule. 
3. NoExecute: node will immediately evict all pods without matching toleration and new pod will not be scheduled on to node. 

apiVerison: v1
kind: Pod
metadata:
  name: nginx
  lables:
    env: prod
spec:
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
  tolerations:
  - key: "app"							
    operator: in
    value: "nginx"
    effect: NoSchedule

adding tolerations: 
1. if the operator is Exists, the value is not needed			
2. Only operator matches all the keys and tolerations. 
3. Empty effect matches all effects. 

----------------Resource limits----------------
By default, Pods run with unbounded CPU and Memory limits. Resource limits prevents single app from using all available resources and protect other apps from resource starvation.
Request and Limits are set for each container. 
Request: what container is gauranted to get	Limits: What container should never exceed  

memory: base unit is bytes (mebibytes) if exceeds memory limit then OOM error and container gets killed
CPU: base units are cores. (1 CPU equal to 1000 millicore) if exceeds cpu limit then apps get throttled. (gives worst performance but not get killed) 

Note: Its duty of scheduler to provide node with requested resources otherwise pod will be in pending state. if only limit specified then request will be limit value. 

Metrics server: collects and exposes resource usage metrics from the cluster nodes and pods.
kubectl top pod
kubectl top node

apiVersion: v1
kind: Pod
metadata:
  name: example-pod
spec:
  containers:
  - name: my-container
    image: nginx:latest
    resources:
      requests:
        memory: "64Mi"   # Requested memory
        cpu: "250m"      # Requested CPU
      limits:
        memory: "128Mi"  # Maximum memory allowed
        cpu: "500m"      # Maximum CPU allowed


------ReqsourceQuota object is applied on namespace------
ResourceQuota is not limited to cpu,memory limits rather its applied for pod,svc,deployments,pvc etc

#create namespace: my-namespace.yml
apiVersion: v1
kind: namespace
metadata:
  name: my-app-namespace

#define resource quota: resource-quota.yml
apiVersion: v1
kind: ResourceQuota
metadata:
 name: my-app-quota
 namespace: my-app-namespace			--> bad practice to hard code namespace rather pass -n <namespace name> while deployment
spec:
 hard:
   pods:"number"
   requests.memory:					Mi --> Mebibytes (more than megabyte 10^6 its decimal unit sotrage but computer understand in binary unit which
   requests.cpu:					is 2^20 little more han megabyte					
   limit.cpu:						m --> millicore
   limit.memory:

Note: If you set resourceQuota for namespace then all the pods need to set the resources in the container definition therwise pod will not be scheduled. 
drawback: obj created within defined namespace can still utilize all available resources starving others. --> LimitRange can overcome this problem by setting min&max limit on obj level in namespace. 

---LimitRange-------
apiVersion: v1
kind: LimitRange
metadata:
  name: example-limit-range
spec:
  limits:
  - type: Container
    default:
      cpu: "500m"          # Default CPU limit for a container (millicores)
      memory: "500Mi"      # Default memory limit for a container (mebibytes)
    max:
      cpu: "2000m"         # Maximum CPU allowed for a container (millicores)
      memory: "2000Mi"     # Maximum memory allowed for a container (mebibytes)
    min:
      cpu: "200m"          # Minimum CPU required for a container (millicores)
      memory: "200Mi"      # Minimum memory required for a container (mebibytes)
  defaultRequest:
    cpu: "100m"            # Default CPU request for a container (millicores)
    memory: "100Mi"        # Default memory request for a container (mebibytes)


-----------------------------K8s Network Policies--------------------https://orca.tufin.io/netpol/ --> policy viewer
Implementing n/w policies critical part of building K8s platform. In K8s by default all pods are non-isolated means ingress and Egress traffic allowed . pods can be isolated by applying n/w policies using ingress rules (like Security Groups)
N/w policies functionality is provided and implemented by network plugins like Calico,Cilium,Flannel,Weavenet etc.

Note: unless you have n/w plugin that implement n/w policy, you will not be able to use functionality. 
1. N/w policies are namespace scoped.
2. Policies are applied to pods using label selector. 
3. If we use NetworkPolicy obj then all traffic is blocked unless specify rules

N/w policy = NetworkPolicyPeer + NetworkPolicyPort

Peer					      port and Protocol
IP Block-------------------------------------->  
pod from diff ns -------Inress traffic-------->  
pod selector (same ns) ----------------------->  POD
nameSpace selector ---------------------------> 

network policy definition comprise of:
1. Pod selector : based on lebels. 
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metdata:
  name: value
spec:
  podSelector: {}				--> empty {} podSelector selects all pods in ns.
  or
  podSelector:
    matchLabels:
      app: example-app			--> label

2. Policy Type: Ingress/Egress
3. Ingress Block
ingress: 				--> [] shows all traffic blocked
 - from: 
    - ipBlock:
        cidr: <ip/range>
        except: <ip/range>
    - namespaceSelector:
	matchLabels:
           key: value
    - podSelector:
        matchLabels:
           key: value
   ports: 
    - protocol:
      port: 
4. Egress Block
 egress: 				--> [] shows all outgoing traffic blocked
 - to: 
    - ipBlock:
        cidr: <ip/range>
        except: <ip/range>
    - namespaceSelector:
	matchLabels:
           key: value
    - podSelector:			--> remove - from podSelector to allow perticular pod from different namespace
        matchLabels:
           key: value
   ports: 
    - protocol:
      port: 

here we are only allowing traffic to pod with db label but when pod want to talk to other pod with db label , it has to talk to service first and service name to ip resolution happens through kube-dns service (core-dns pod) under kube-system ns but we have not inlcuded any rule for pod to talk to kube-system ns pods. so its egress rule fail. below rule make egress to work:
 egress: 				
 - to: 
    - namespaceSelector:
	matchLabels:
           kubernetes.io/metadata-name: kube-system
    - podSelector:			
        matchLabels:
           app: db
   ports: 
    - protocol: UDP
      port: 53
    - protocol: TCP
      port: 80

-------------------------------------Static Job--CronJob--daemonSet-------------

Static pod --> A pod directly managed by kubelet on the node ( created in manifest location on worker node /etc/kubenetes/manifests/)  but not by K8s API server. They can run even if there is not any K8s API server. all control-plane components run as static pods. 
Kubelets automatically creates static pods from YAML manifest files (kept in /etc/kubenetes/manifests/) on master/worker node. 
kubectl get po  --> shows <podname>-master-node			--> identify static pods 

Mirror pod ---> Kubelet will create a mirror pod  for each static pod. Mirror pod allow you to see the status of Static pods via K8s API , but you cannot change 
or manage them via API server. 
you cannot delete static pod by running kubectl delete. to delete it remove yaml file from manifest location. 

DaemonSet(kind) --> Makes sure that each pod replica running on each node. Automatically runs a copy of pod on each node. DaemonSet will run copy of pod on new node when they are added to the cluster. DaemonSet normally respect scheduling rules around nodeLabels, taints and tolerations. If pod would not normally schedule on node, daemonSet will not create copy of pod on that node.

ex: cluster storage daemon on each node: ceph,glusterd
log collection agent daemon: fluentd,logstash
node monitoring agent daemon: node exporter(prometheus), collectd

K8s Jobs:
Job is k8s resource that generally used for large computation and batch tasks. all jobs create one or more pods and are run to completion(exit with 0 status code) 

apiVersion: batch/v1
kind: Job
metadata:
  name: example-job
spec:
  parallelism: 2			---> number of pods to run concurrently
  completions: 4			--> desired number of successfully completed pods before considering the job as succeeded
  backoffLimit: 3			--> maximum number of retries before considering the job as failed
  ttlSecondsAfterFinished: 60		--> amount of time to retain completed pods after the Job has finished. jobs will not delete automatically
  template:
    metadata:
      name: example-pod
    spec:
      containers:
      - name: example-container
        image: busybox:alpine
        command: ["/bin/sh"]
        args:
        - "-c"
        - "num=$((RANDOM % 10)); if [ $num -eq 0 ]; then echo 'positive'; exit 0; else echo 'negative'; exit 1; fi"
  restartPolicy: OnFailure/Never


CronJob: similar to a Job but adds the ability to schedule periodic or recurring tasks.
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: example-cronjob
spec:
  schedule: "*/1 * * * *"  # Run every minute
  successfulJobsHistoryLimit: 3			-->  number of completed jobs to keep in history. 
  failedJobsHistoryLimit: 1			--> number of failed jobs to keep
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: example-app
        spec:
          containers:
          - name: example-container
            image: busybox:alpine
            command:
            - "/bin/sh"
            args:
            - "-c"
            - "echo Hello from the CronJob!"


apiVersion: batch/v1
kind: CronJob
metadata:
   name: pwd-runner
   namespace: one
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      activeDeadlineSeconds: 10						--> terminate cronjob if failed to execute within 10s
      template:
        metadata:
          labels:
            name: pwd-runner
        spec:
          containers:
          - name: batch
            image: busybox:stable
            command: ["/bin/sh"]
            args:
            - "-c"
            - "pwd"
          restartPolicy: OnFailure

Probes: are mechanisms used to determine the health of a container. 3 types
1. Liveness Probe: Checks whether the container is still running. If the liveness probe fails, K8s restarts the container.

2. Readiness Probe: Determines whether the container is ready to serve traffic. If the readiness probe fails, the container is removed from the service endpoints, and traffic is not directed to it.

3. Startup Probe: Similar to the liveness probe, but it only runs during the initial startup of a container. It is used to delay the liveness and readiness probes until a container has performed some necessary initialization.

apiVersion: v1
kind: Pod
metadata:
  name: probe-example
spec:
  containers:
  - name: my-container
    image: nginx:latest
    livenessProbe:			--> can be readinessProbe/startupProbe
      httpGet:				--> can be exec,tcpSocket,Grpc etc
        path: /healthz
        port: 80
      initialDelaySeconds: 3		--> number of seconds k8s should wait before starting the liveness probe
      periodSeconds: 3			--> Specifies the period (in seconds) at which the liveness probe should be performed. every 3 sec
      

----------------------Persistent Volumes----------------------------
With persisted volume(PV) data is persisted regardless of apps lifecycle container. pod,node or even cluster itself. 
PV consist of 2 things
1. backend storage called persistentVolume: 
   2 ways to provision as Persistent Volume. 
	1. Static: manually created by admin
 	2. Dynamic: cluster creates pv based on storageClassName(backend storage. i.e ebs,efs or nfs etc) for pvc. if not given then K8s use deafult storageClasses. 
2. access mode --> how the volume should be mounted. 
	1. ReadWriteOnce: Only single worker node can mount at a time. >1 pod from same node can RW.  
	2. ReadOnlyMany: can be mounted as read only by many nodes at a time
	3. ReadWriteMany: can be mounted as read write by many nodes at a time. 

PVC: PersistentVolumeClaim (PVC) is an abstraction that represents a request for storage by a user or a pod. It allows users to request specific resources in terms of storage, and it decouples the definition of storage requirements from the underlying storage implementation.
each claim can request size and access modes. 

Reclaim Policy(persistentVolumeReclaimPolicy): tells us what happens to PV when PVC is deleted. 
1. delete: deletes volume content and makes volume available to be claimed again by other pvc  				--> default if not specified
2. Retain: PV content is persisted even after pvc deleted. it can't be reused until admin manually reclaim it. 

volume Binding mode:
1. Immediate: provisioned pv and bind immediately once PVC created
2. WaitForFirstConsumer:  delay provisioning and binding of pv until pod using pvc created. 

Note: PVC and PV has one-one mapping. 

apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /data
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    image: my-image
    volumeMounts:
    - name: my-storage
      mountPath: /app/data
  volumes:
  - name: my-storage
    persistentVolumeClaim:
      claimName: my-pvc

Note: When you create the PVC, and if a matching StorageClass is specified, K8s looks for a provisioner associated with that StorageClass.
The provisioner dynamically provisions a new PersistentVolume with the requested capacity,access modes,binding mode etc"

---------------------Auto Scaling------------------
Horizontal Pod Autoscaler (HPA): Automatically adjusts the number of replicas in a deployment or replica set based on observed metrics.
various metrics types:
1. Resource metrics: Scale based on cpu or memory utilization
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment					
    name: nginx-deployment
  minReplicas: 2
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu/memory
      target:
        type: Utilization
        averageUtilization: 50

2. Customm metrics
3. External metrics: Deploy an external metrics provider that can expose external metrics to K8s.
4. Pod-based metrics
5. Object metrics

Scaling Policies:
Target Utilization: scale to % CPU utilization
Target Value:  scale t target metric value (e.g., maintain 100 requests per second).
Average Value: Scale to achieve an average value across multiple pods.
 
VPA: Automatically adjusts the resource requests of individual pods.

------------------------StatefulSets--------------------
Headless services are mainly used to connect to database cluster However, in headless service if pods gets restaretd then their IP's and DNS(which also includes IP's like IP.service.ns.svc.cluster.local) also get changed so it will be challenging job to connect to dedicated backend pod.

we can use statefulSets plus headless service to fix the above issue where pods in statefulsets gets stable DNS 

statefulSets are valuable for applications that require one or more of the following. 
1. Stable,unique network identifiers with help of headless service. 
2. Stable persistent storage
3. Ordered,graceful deployment and scaling 
4. Ordered, automated rolling update. 

#statefulSet manifest file is same as deployment with addition of volumeClaimTemplates. 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: example-statefulset
spec:
  serviceName: "example-statefulset"			--> name of the headless service that the StatefulSet controls.
  replicas: 3
  selector:
    matchLabels:
      app: example-app
  template:
    metadata:
      labels:
        app: example-app
    spec:
      containers:
      - name: web
        image: nginx:latest
        ports:
        - containerPort: 80
  volumeClaimTemplates:					--> (PVCs) that will be created for each pod
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi

-----------------------------------------SSL Certificate-------------------------
CA: certificate authority: signs cert with its own private key so cert can be decrypted using available public key. CA public keys are available with all browsers.
CA'		   			Server 		server
private key	-->    sign         --> public key ---> Certificate

SSL handshake:
1. user enters the url to initiate connection
2. Server shares its CA Cert with public key
3. Browser verifies the SSL Cert
4. Browser shares the session key enctrypting it with server cert
5. server decrypt data and receives  the shared key
6. Brower and server share the enctrypted dara using shared key


--------------------------------RBAC-------------------------
RBAC is security feature in K8s lets you create fine grained permissions to manage what actions users and workloads can perform on resources in your cluster. 
Default admin user has full access to K8s resources. but not every user needs unrestricted access. so RBAC used to restrict access to K8s cluster based on role.
RBAC policies used to manage access rights of system user (user and groups) and service accounts. 

User				Role						clusterRole
david	--> role binding---> dev-role (can access all resources in dev ns)
estia	--> role binding --> test-role (can access all resrces in test ns)

admin  --------------------clusterRoleBinding----------------------------------> admin-cr (full access to cluster)
serviceAccount(Pod) -------clusterRoleBinding-----------------------------------> monitor-cr (read only access to all resources in cluster)


2 types of roles: assigned to users,groups and service accounts. 
1. role		--> sets permission for specific ns. 
2. clusterRole  --> define permission for whole cluster. 

When we bind role to accounts(user,group or service account) called roleBinding. 


Resources vs Verbs:
In K8s we are interested in controlling access to resources such as pods,services,endpoints,deployment,Ingress etc
Verbs are permisisons like read-only,write-ony etc (get,list,watch,create,patch,,update,delete) 

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: development
  name: developer-role
rules:					--> Defines the permissions for this Role, 
- apiGroups: [""]			--> specifies the API groups to which the resources belong. An empty string ("") represents the core API group
  resources: ["pods","pods/log","pods/exec"]
  verbs: ["get", "list"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: developer-role-binding
  namespace: development
subjects:							--> Specifies the user (or users) to whom the Role is bound
- kind: User
  name: john@example.com   # Replace with the actual username
  apiGroup: rbac.authorization.k8s.io
roleRef:							--> References the Role (developer-role) to bind.
  kind: Role
  name: developer-role
  apiGroup: rbac.authorization.k8s.io

-------------------RBAC Analyser-------------------------------------------
RBAC permissions audit tool that help you to  scan your whole cluster to locate any risky permissions.
RBAC analysis tools:
1. kubescan
2. kubescape
3. Krane
4. RBAC-Tool
5. RBAC-View

clone repo:
git clone https://github.com/cyberark/KubeScan.git
pip install kubernetes PTable
alias kubescan='python KubeScan/kubescan.py'

Get Help:
python KubeScan/kubescan.py -e

kubescan --risky-roles
kubescan --risky-clusterroles
kubescan --risky-any-roles
-------------------------------------- 



Self draining K8s node: 
when performing maintainance, you may sometimes need to remove K8s node from service. To do this, you can drain the node. 
Containrs running on node will be gracefully terminated (and potentially rescheduled to another node)
kubctl drain node-name
kubectl drain node-name --ignore-daemonsets   (when draining a node you may need to ignore daemonsets i.e pods that are tied to each node. If you have any daemonset pods running on node then likely use --ignore-daemonset flag.

uncordon node: 
If node remains part of cluster, You can allow pods to run on node again when maintanance is complete using 
kubectl uncordon node-name
Note: uncordoning the node will not guarantee that pods get restored on same node again. 

backig up and restoring ETCD cluster data:
ETCD is backed data storage solution as such all K8s objects, apps and configurations are stored in etcd. so likely to backup cluster data by backing up etcd.
ETCDCTL_API=3 ectdctl --entpoints $ENDPOINT snapshot save <file-name>
ETCDCTL_API=3 etcdctl snapshot restore <file-name>    --> you need to supply additional parametrs as restore operation creates a new logical cluster.

K8s object management:
you can use the kubectl to deploy applications, inspect and manage cluster resources and view logs. 
kubectl get <object type> objectname -o <output-yaml/json/wide> --sort-by <JSONPath> --selector <label>
kubectl create -f <filename>   --- if we try to create object already exist an error will occur.
kubctl apply -f <filename>     ---same as create but if we apply on already existed object then it will modify.
kubectl delete <object type> ojbect-name
kubectl exec <pod-name> -c <container-name> -- <command>   -->used to run commands inside container.if pod has multiple conainers then specifiy container with -c otherwise optional.

kubectl declarative commands --> define object using the data structure like yaml/json. kubectl apply -f <filename>
kubectl imperative commands --> define objects using kubectl commands. kubectl create deployment my-deployment -- image=nginx --dry-run -o yaml. X create pods using it.
kubectl scale deployment my-deployment replica=5 or --replica 5 --record     ---> record a command

RBAC in K8s:It allows you to control what users are allowed to do and access within your cluster. example: we can use RABC to allow dev to read metadata and logs from
K8s pods but not make any changes to them. 

Cluster----------------------------------------    Roles and ClusterRoles are K8s objects that define a set of permissions. Role define permissons within perticuler
|                                             |    namespace and cluster role defines a cluster wide permissions not specific to single namespace.
|   Namespace---------     ClusterRole------  |
|   |  Role          |    |  permissions   |  |
|   |  permissions   |    |                |  |    RoleBinding and ClusterRoleBinding are objects that connect roles and cluster roles to users.
|   |                |    |                |  |
|   |______ ↑________|    |_______ ↑_______|  |
|                                             |
|     RoleBinding         ClusterRoleBindding |
____________ ↑______________________ ↑________|           
                     ↑
             Users/ServiceAccounts

K8s Service Account: In K8s, Service account is an account used by container processes within pods to authenticate with K8s API.if your pods needs to communicate with 
K8s API then you can use service accounts to control their access. A service account object can be created with some YAML just like other K8s object.
apiVersion: v1
kind: ServiceAccount
metadata: 
 name: my-serviceaccount

you can manage access control for service accounts just like any other user, using RABC objects. 

Inspecting pod resource usage: 
Kubernates metrics server: In order to view metrics about the resources, pods and containers are using, we need an add-on to collect and provide that data. One such 
add-on is kubernates metrics server. Once we installed that add-on we can use top command to view data about resource usage in pods and nodes.  
kubectl top pod --sort-by <JSONpath> --selector <label>


Rolling update: allows yout o make changes to deployment pods at controlled rate, gradually replacing old pods with new one. This allows you to update your pods without incussing 
downtime. 
kubectl rollout status deployment.v1.apps/my-deployment  --> to see how rolling update is happening
kubectl set image deployment/my-deployment nginx=<version> --record      ----> command for rolling update
Rollback: If an update to the deployent cause a problem , yoyu can roll back the deployment to the previous working state. 


Each service has type and Service Type determines how and where the service will expose your application. There are 4 types,
1. ClusterIP  --> expose service only within cluster network
2. NodePort  --> expose service outside the cluster network. (should be between 30000-32767) not suitable for prod use case. 
3. LoadBalancer  --> also expose service outside of cluster network. But they use an external cloud load balancer to do so. works only with cloud platform.
4. Headless service -> used for statefullsets where each pod is not identicle. set  ClusterIp=none to use headless service. so app pod can directly connect using pod ip address.



--------------------common kubectl commands----------------------
Basic Commands:
kubectl get: Display one or many resources.
kubectl describe: Show details of a specific resource.
kubectl create: Create a resource from a file or from stdin.
kubectl apply: Apply a configuration to a resource.

Viewing Resources:
kubectl get pods: List all Pods in the current namespace.
kubectl get services: List all Services in the current namespace.
kubectl get deployments: List all Deployments in the current namespace.


Interacting with Pods:
kubectl logs <pod-name>: Print the logs from a Pod.
kubectl exec -it <pod-name> -- /bin/bash: Start an interactive shell in a Pod.


Managing Resources:
kubectl delete: Delete resources by filenames, stdin, resources, and names, or by resources and label selector


Managing Configuration:
kubectl config: Modify kubeconfig files.


Troubleshooting:
kubectl describe pod <pod-name>: Show details of a Pod, including events.
kubectl get events: Display events related to Pods and other resources.


Cluster Info:
kubectl cluster-info: Display addresses of the master and services.


Contexts:
kubectl config get-contexts: Display available contexts.


Help:
kubectl --help: Display help for kubectl and its subcommands.



------------------------------Interview Questions-----------------------------------
1. what is difference between docker and kubernetes? 
docker is open source containerized platform whereas K8s is container orchestration env.
docker main purpose is to solve code portability issue and k8s offeres capabilities like deployment,auto scaling, load balancing etc 

2. what are main components of K8s?
on broad level we cna devide k8s components into 2 parts
1. control plane
2. data plane or worker nodes

control plane consist of components like Kube API Server,Scheduler,Kube Controller manager,etcd and cloud controller manager
whereas data planes consist of kubelet,kubeproxy and containerd 


3. what are the main diffences between docker swarm and kubernetes?
1. Managed Services:
Docker Swarm: While Docker Swarm itself is easy to install, it lacks widely adopted managed services from cloud providers. This means organizations using Docker Swarm might need to invest more effort in managing the control plane themselves.

Kubernetes: Kubernetes, being the more popular choice, has managed services offered by major cloud providers such as Google Kubernetes Engine (GKE), Amazon EKS, and Azure Kubernetes Service (AKS). These managed services reduce the operational overhead of managing the control plane, allowing teams to focus more on application development and deployment.

2. GUI for Interactive Orchestration:
Docker Swarm: Docker Swarm does not have a built-in graphical user interface (GUI) for interactive orchestration.
Kubernetes: Kubernetes often leverages tools like Kubernetes Dashboard to provide a graphical interface, making it easier for users to visualize and manage resources 
within the cluster.

3. Package Manager:
Docker Swarm: Docker Swarm does not have its own package manager like Helm.
Kubernetes: Kubernetes has Helm, a widely used package manager, which simplifies the deployment and management of applications through pre-configured packages called charts.

4. Integration with Third-Party Tools:
Docker Swarm: While Docker Swarm is easy to install, it may lack extensive integration with third-party tools and plugins commonly used in the Kubernetes ecosystem.
Kubernetes: Kubernetes has a rich ecosystem with many third-party tools and plugins, including monitoring tools like Prometheus and Grafana, logging solutions like EFK (Elasticsearch, Fluentd, Kibana), and continuous delivery tools like ArgoCD. This makes Kubernetes more versatile and adaptable to various use cases.

5. Community and Industry Standard:
Docker Swarm: Docker Swarm has a smaller community compared to Kubernetes, and it is not as widely considered an industry standard for container orchestration.
Kubernetes: Kubernetes has a large and active community, making it an industry-standard tool with widespread adoption and support.

6. swarm has basic auto scalling like replica and glabal Whereas K8s supports advance auto scaling concepts like HPA,cluster auto scaler,karpenter etc


4. what is namespace in k8s?
namespaces provide a way to partition cluster resources between multiple users, teams, or applications. You can use namespaces to create isolated environments. 


5. what is kube-proxy?
is network proxy. It runs on each node and provides networking between containers and services(single entrypoint which manages backend nodes) in the cluster by maintaining iptable


6. How 2 containers inside a pod communicate with each other?
1. localhost communication: 
when you have multiple containers running inside the same pod, they share the same network namespace. and containers can talk to each other using localhost or loopback address and container port. "pause container" plays crutial role here for holding the network namespace open and keeping the network namespace of the pod alive.

2. service discovery: Containers can use Kubernetes services for service discovery. Each container exposes its service on a specific port, and other containers refer to it using the service name and port.


7. what is pause container?
pause container: its design pattern used to share the n/w namespace among container in pod for internal container communication plus ensure that IP address remains constant when container restart. it's automatically injected into each pod.


8. what is flannel?
open-source networking solution designed for container orchestration. It is commonly used in Kubernetes clusters to provide a simple and scalable overlay network for container communication. there are other Kubernetes networking solutions available, such as Calico, Weave, and Cilium, each with its own features and characteristics.

Flannel: simple and quick setup, small scale deployment, lower env
Calico: Advance network policies (strict security,n/w policy), Large scale deployments, strong communitiy, prod etc
EKS use VPC-CNI plugin

9. How the 2 pods communicate with each other?
Service discovery: through service dns name or clusterIP
pod-pod communication through IP (less common)

by default pods in different ns cannot talkt o each other. if they want to communicate the use service FQDN. pod-1 can talk to pod-2 using pod-2 FQDN 
curl pod-2.ns2.svc.cluster.local


10. what is master and manion?
In the context of Kubernetes, the terms "master" and "minion" have been historically used, but more commonly, you'll hear the terms "control plane" and "worker nodes"

11. How would you test the manifest without actually executing it?
we can us --dry-run=client -o yaml option 


12. How do you initiate a rollback for an application?
we can check the history of changes made using rollout command:
kubectl rollout history deploy <deploy-name>			--> it will show me revision and change cause
kubectl rollout undo deploy <deploy-name>			--> only dry run
kubectl rollout undo deploy my-deployment --to-revision=3 --record --dry-run='false' --save-config --annotation="rollback.reason=FixingIssue123"

Note: without --dry-run='false' it only simulate rollback but will not rollback in actual. --record log the changes to history --annotation helps to provides cause 


13. How can I access containers running inside pod from browser ?
To access containers running inside a pod from your local browser, you can use the kubectl port-forward command to create a secure tunnel between your local machine and the pod.
kubectl port-forward pod/<pod-name> [local-port]:[pod-port]


14. How to configure default ImagePullSecret for any deployment?
To attach an image pull secret to a service account in Kubernetes, you can create or update the service account to include the desired image pull secret. This is useful when you want to avoid putting the image pull secret directly in individual deployments or replica sets.
1. Create a Docker Config JSON File:
my-secret.json
{
  "auths": {
    "your-registry-url": {
      "username": "your-username",
      "password": "your-password",
      "email": "your-email"
    }
  }
}

2. create k8s secret
kubectl create secret generic my-image-pull-secret --from-file=.dockerconfigjson=my-secret.json --type=kubernetes.io/dockerconfigjson

3. attach secret to SA
kubectl patch serviceaccount default -p '{"imagePullSecrets": [{"name": "my-image-pull-secret"}]}'


15. What is the process of granting access to an EKS cluster for a new user?
1. Create an IAM user or group: provide necessary permission to access k8s/eks cluster 
2. configure aws cli
3. eks cluster access: 
	1. IAM role mapping with aws-auth config map: Update the aws-auth ConfigMap in the kube-system namespace to map the IAM user or group to a K8s RBAC role
	apiVersion: v1
	kind: ConfigMap
	metadata:
  		name: aws-auth
  		namespace: kube-system
	data:
  	   mapUsers: |
    	   - userarn: arn:aws:iam::ACCOUNT-ID-WITHOUT-HYPHENS:user/USERNAME
      	     username: USERNAME
      	     groups:
           - system:masters
	2. update kubeconfig file
	aws eks update-kubeconfig --name your-cluster-name
4. K8s RBAC setup
setup SA and roleBinding (optional)	--> If you want to further restrict access using Kubernetes RBAC, create a Kubernetes Service Account for the new employee and create a RoleBinding associating the Service Account with specific roles.

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: employee-role
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch", "create", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: employee-binding
  namespace: default
subjects:
- kind: User
  name: USERNAME
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: employee-role
  apiGroup: rbac.authorization.k8s.io


16. I have created a pod but status is pending. how to debug?
We can use the kubectl describe command to check pods detailed info which includes state,label,volume and events etc
kubectl describe pod pod-name 



17. say dev team has deployed nginx web server successfully, but webpage is not accessibe. how to you debug?
1. check pod events by describe command
2. check the svc details whether service match the deploy labels
3. check server logs usig kubectl logs -f pod-name  
4. if everything looks good then check what error you are getting while accessing web page (like connection refuse)
5. check web server config by kubectl exec command		--> found server post and svc port are different 


18. what is daemonset and use case of daemonset?
DaemonSet if K8s object which makes sure that each pod replica running on each node. example: log collecto agents like fluentbit,monitoring agents like node-exporter


19. How to automate K8s app?
tell the jenkins pipeline steps

20. How to secure K8s?
K8s security
	1. Applicaion security
		Pod,namespace,node
		RBAC,IRSA
	2. DevSecOps: 
		authorization (IAM roles)
		Scan images through linting tools like trivy
		Scan running containers (AWS ECR vul scanning)

21. How to cost/performance optimize K8s apps?
Control plane cost(fixed not much room for improvement)
worker node numbers and types:
	1. Pod resource specification
	2. Unused CPU or memory allocation
	3. Detect CPU/Memory waste (install metrics server)


22. Tell me challenge you faced in K8s apps?
A challenge faced was upgrading a Kubernetes cluster to a new version on EKS with hundreds of nodes. The challenge included creating and rehydrating AMIs while keeping the application highly available and maintaining pod disruption budget. 
The solution involved using EKS managed node groups,AWS provides patches AMIs, app wil be up and running with high availability, respect the pdo disruption budget.

2nd challenge was related to IP address scarcity:
every pod uses the Ip address from VPC and as apps grows we run short of IP's so solution is we need to add subnet to the existing node group vpc so increase the IP count. 


what is pod disruption budget?
A Pod Disruption Budget (PDB) is a policy in Kubernetes that allows you to control the disruption caused by voluntary disruptions, like scaling down, node maintenance or node upgrades. it specifies the maximum number or percentage of pods that can be simultaneously disrupted (unavailable) during voluntary disruptions.

apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: example-pdb
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: example		--> label of pods
 
it acts as safety mechanism to prevent aggressive scaling-down actions. 

